diff --git a/.secrets.baseline b/.secrets.baseline
index 0ef48200..7a3e4b16 100644
--- a/.secrets.baseline
+++ b/.secrets.baseline
@@ -90,6 +90,10 @@
     {
       "path": "detect_secrets.filters.allowlist.is_line_allowlisted"
     },
+    {
+      "path": "detect_secrets.filters.common.is_baseline_file",
+      "filename": ".secrets.baseline"
+    },
     {
       "path": "detect_secrets.filters.common.is_ignored_due_to_verification_policies",
       "min_level": 2
@@ -123,36 +127,43 @@
     }
   ],
   "results": {
-    "features/test_starter/{{ cookiecutter.repo_name }}/conf/local/credentials.yml": [
+    "kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml": [
       {
         "type": "Secret Keyword",
-        "filename": "features/test_starter/{{ cookiecutter.repo_name }}/conf/local/credentials.yml",
-        "hashed_secret": "a62f2225bf70bfaccbc7f1ef2a397836717377de",
+        "filename": "kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml",
+        "hashed_secret": "e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4",
         "is_verified": false,
-        "line_number": 8
+        "line_number": 9
       },
       {
         "type": "Secret Keyword",
-        "filename": "features/test_starter/{{ cookiecutter.repo_name }}/conf/local/credentials.yml",
+        "filename": "kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml",
         "hashed_secret": "d033e22ae348aeb5660fc2140aec35850c4da997",
         "is_verified": false,
-        "line_number": 16
+        "line_number": 18
       }
     ],
-    "kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml": [
+    "tests/cache/test_step_cache.py": [
       {
         "type": "Secret Keyword",
-        "filename": "kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml",
-        "hashed_secret": "e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4",
+        "filename": "tests/cache/test_step_cache.py",
+        "hashed_secret": "f2b14f68eb995facb3a1c35287b778d5bd785511",
         "is_verified": false,
-        "line_number": 9
+        "line_number": 125
       },
       {
         "type": "Secret Keyword",
-        "filename": "kedro/templates/project/{{ cookiecutter.repo_name }}/conf/local/credentials.yml",
-        "hashed_secret": "d033e22ae348aeb5660fc2140aec35850c4da997",
+        "filename": "tests/cache/test_step_cache.py",
+        "hashed_secret": "cba7d8d926ee837cf3f92636a6d1cd0eb397db64",
         "is_verified": false,
-        "line_number": 18
+        "line_number": 131
+      },
+      {
+        "type": "Secret Keyword",
+        "filename": "tests/cache/test_step_cache.py",
+        "hashed_secret": "e5e9fa1ba31ecd1ae84f75caaa474f3a663f05f4",
+        "is_verified": false,
+        "line_number": 141
       }
     ],
     "tests/config/test_omegaconf_config.py": [
@@ -206,5 +217,5 @@
       }
     ]
   },
-  "generated_at": "2025-07-17T13:57:33Z"
+  "generated_at": "2026-01-28T04:39:31Z"
 }
diff --git a/CACHE_DESIGN.md b/CACHE_DESIGN.md
new file mode 100644
index 00000000..2a82f5b6
--- /dev/null
+++ b/CACHE_DESIGN.md
@@ -0,0 +1,375 @@
+# Pipeline Caching Design — Kedro
+
+## 1. Architecture Diagram
+
+```
+┌─────────────────────────────────────────────────────────────────────────┐
+│  KedroSession.run()                                                     │
+│    └── hook_manager.hook.before_pipeline_run()                          │
+│            └── CachingHook.before_pipeline_run()                        │
+│                    └── Load CacheRegistry from backend                  │
+│                    └── Walk DAG → compute code_hash + param_hash        │
+│                    └── Store "nodes_to_skip" set on hook instance       │
+│                                                                         │
+│  AbstractRunner._run()  ← MODIFIED                                      │
+│    └── for each node (topo order):                                      │
+│          ├── _is_node_cached(node, catalog, hook_manager)  ← NEW        │
+│          │     ├── If CachingHook not registered → False (no-op)        │
+│          │     ├── Load inputs from catalog (for input_hash)            │
+│          │     ├── Compare current StepCache vs stored StepCache        │
+│          │     ├── Log specific field on miss                           │
+│          │     └── Return True (skip) or False (execute)                │
+│          │                                                              │
+│          ├── [CACHE HIT]  → skip, add to done_nodes                    │
+│          │                                                              │
+│          └── [CACHE MISS] → Task.execute()                              │
+│                └── node.run(inputs)                                     │
+│                └── catalog.save(outputs)                                │
+│                └── hook_manager.hook.after_node_run()                   │
+│                        └── CachingHook.after_node_run()                 │
+│                                └── with cache_transaction():           │
+│                                        └── verify output exists         │
+│                                        └── registry.set(step_cache)     │
+└─────────────────────────────────────────────────────────────────────────┘
+
+┌─────────────────────────────────────────────────────────────────────────┐
+│  Cache Registry Backends                                                │
+│                                                                         │
+│  ┌─────────────────────┐    ┌─────────────────────┐                    │
+│  │  RedisBackend       │    │  FilesystemBackend  │                    │
+│  │  (default)          │    │  (fallback)         │                    │
+│  │  - HSET/HGET/HDEL   │    │  - JSON read/write  │                   │
+│  │  - Atomic ops       │    │  - File lock        │                    │
+│  │  - Cross-process    │    │  - Project-local    │                    │
+│  └─────────────────────┘    └─────────────────────┘                    │
+└─────────────────────────────────────────────────────────────────────────┘
+
+┌─────────────────────────────────────────────────────────────────────────┐
+│  CodeHasher (Lazy, Memoized)                                            │
+│                                                                         │
+│  hash_function(func)                                                    │
+│    └── resolve source file                                              │
+│    └── check _file_cache[(path, mtime)] → return cached hash if hit    │
+│    └── parse AST → strip comments/whitespace                            │
+│    └── follow project-local imports (recursive, same mtime check)       │
+│    └── combine sorted hashes → SHA-256                                  │
+│    └── store in _file_cache                                             │
+│                                                                         │
+│  _file_cache: dict[(path, mtime) -> hash]   # singleton per process    │
+│  _hash_call_count: int                      # for test assertions      │
+└─────────────────────────────────────────────────────────────────────────┘
+```
+
+## 2. Module List
+
+| Module | Action | Rationale |
+|--------|--------|-----------|
+| `kedro/cache/__init__.py` | Create | Public API surface for the cache subsystem |
+| `kedro/cache/step_cache.py` | Create | StepCache dataclass, Environment, CacheInvariant, validation, serialization |
+| `kedro/cache/code_hasher.py` | Create | AST-based code hashing with per-file mtime memoization |
+| `kedro/cache/registry.py` | Create | CacheRegistry ABC; lookup/store/delete operations |
+| `kedro/cache/backends/__init__.py` | Create | Backend base class exports |
+| `kedro/cache/backends/redis_backend.py` | Create | Redis-backed persistent registry (default) |
+| `kedro/cache/backends/filesystem_backend.py` | Create | JSON file-backed registry (fallback, used in tests) |
+| `kedro/cache/hook.py` | Create | CachingHook — implements `before_pipeline_run`, `before_node_run`, `after_node_run`. Orchestrates cache check + registration lifecycle |
+| `kedro/runner/runner.py` | Modify | Add `_is_node_cached()` check in both sequential and executor-based loops inside `_run()`. Zero effect when no CachingHook is registered |
+| `pyproject.toml` | Modify | Insert `cache` layer in import-linter contracts; add exception for `kedro.cache.hook -> kedro.framework.hooks`; add `redis` to `[test]` extras |
+
+## 3. StepCache Data Model
+
+```python
+@dataclass(frozen=True)
+class StepCache:
+    # --- Step Info ---
+    step_id: str                          # node.name (unique within pipeline)
+    start_timestamp: str                  # ISO-8601
+    end_timestamp: str                    # ISO-8601
+    session_id: str                       # KedroSession.session_id
+    worker_id: str                        # os.environ["KEDRO_WORKER_ID"] or "main"
+    cache_hits: int = 0                   # incremented on each cache hit
+
+    # --- Environment ---
+    runner_class: str                     # fully qualified class name
+    pipeline_namespace: str | None        # node.namespace
+    cli_flags: dict[str, Any]             # _jsonify_cli_context(), masked
+    config_env: str                       # context.env (e.g. "local")
+    config_source: str                    # "base" | "local" | "both"
+
+    # --- Cache Invariants (what triggers invalidation) ---
+    code_hash: str                        # SHA-256 of AST (transitive, lazy)
+    input_hash: str                       # SHA-256 of sorted serialized inputs
+    parameter_hash: str                   # SHA-256 of sorted serialized params
+
+    # --- Input Metadata ---
+    input_paths: dict[str, str]           # dataset_name -> relative filepath (if persisted)
+    input_schema: dict[str, str]          # dataset_name -> repr of schema/type
+
+    # --- Output Metadata ---
+    output_paths: dict[str, str]          # dataset_name -> relative filepath
+    save_version: str | None              # DataCatalog save_version
+```
+
+### Cache Validation Fields (compared current vs stored)
+Only these fields cause invalidation:
+- `runner_class`, `pipeline_namespace`, `config_env` (Environment)
+- `code_hash`, `input_hash`, `parameter_hash` (Invariants)
+- `input_paths` keys (Input structure)
+- Output must exist on disk
+
+`session_id`, `worker_id`, `start_timestamp`, `cache_hits` are metadata — they do NOT cause invalidation.
+
+### Downstream Propagation
+The hook walks nodes in topological order. If node A is a cache miss, all nodes reachable from A in the DAG are added to a `_dirty_nodes` set regardless of their own cache state. This set is checked during the runner loop.
+
+## 4. Credential Masking
+
+Blocklist strategy. Any CLI flag key containing (case-insensitive) one of:
+```
+password, secret, token, credential, api_key, private_key, auth, passwd, pwd
+```
+has its value replaced with `"***REDACTED***"`. The blocklist is a class-level constant, configurable by subclassing or passing a custom set.
+
+## 5. Code Hasher Design
+
+```python
+class CodeHasher:
+    """Singleton-per-process. Hashes function source + transitive project imports via AST."""
+
+    _file_cache: ClassVar[dict[tuple[str, float], str]] = {}
+    _hash_call_count: ClassVar[int] = 0  # exposed for test assertions
+
+    @classmethod
+    def hash_function(cls, func: Callable) -> str:
+        """Entry point. Returns SHA-256 hex digest."""
+
+    @classmethod
+    def _hash_file(cls, filepath: str, project_root: str) -> str:
+        """Hash single file. Returns cached result if mtime unchanged."""
+        # 1. stat(filepath) -> mtime
+        # 2. if (filepath, mtime) in _file_cache: return cached
+        # 3. _hash_call_count += 1
+        # 4. ast.parse(source)
+        # 5. _ASTNormalizer visit (strips comments, normalizes whitespace-insensitive repr)
+        # 6. Collect project-local imports from AST
+        # 7. For each import: resolve to filepath, recurse _hash_file
+        # 8. Combine: SHA-256(own_ast_repr + sorted(imported_hashes))
+        # 9. Cache and return
+
+    @classmethod
+    def reset(cls) -> None:
+        """Clear cache. Used in tests."""
+```
+
+Key properties:
+- A file is parsed **at most once** per (filepath, mtime) pair per process lifetime
+- `_hash_call_count` increments only on actual AST parse (cache miss), not on cache hit
+- Only project-local imports are followed (heuristic: file resolves under `project_root`)
+
+## 6. Context Manager: Transactional Cache Registration
+
+```python
+@contextmanager
+def cache_transaction(registry: CacheRegistry, step_cache: StepCache):
+    """
+    Ensures the cache entry is written ONLY after the dataset save succeeds.
+    A sentinel key prevents stale cache reads if the process dies mid-save.
+    """
+    sentinel = f"{step_cache.step_id}.__pending__"
+    try:
+        registry.set_pending(sentinel)       # mark "saving in progress"
+        yield                                # caller does catalog.save() here
+        registry.delete(sentinel)            # save succeeded
+        registry.set(step_cache.step_id, step_cache)  # register cache
+    except Exception:
+        registry.delete(sentinel)            # clean up sentinel
+        raise                                # do NOT register cache on failure
+```
+
+On registry load, any key ending in `.__pending__` is treated as incomplete and ignored.
+
+## 7. Runner Integration (Minimal Diff)
+
+### Sequential path (pool is None):
+
+```python
+# EXISTING:
+for exec_index, node in enumerate(nodes):
+    try:
+        Task(...).execute()
+        done_nodes.add(node)
+
+# BECOMES:
+for exec_index, node in enumerate(nodes):
+    try:
+        if _is_node_cached(node, catalog, hook_manager, run_id, dirty_nodes):
+            done_nodes.add(node)
+            self._logger.info("Skipped node (cache hit): %s", node.name)
+            continue
+        Task(...).execute()
+        done_nodes.add(node)
+```
+
+### Parallel/executor path:
+
+```python
+for node in ready:
+    if _is_node_cached(node, catalog, hook_manager, run_id, dirty_nodes):
+        done_nodes.add(node)
+        continue
+    task = Task(...)
+    futures.add(executor.submit(task))
+```
+
+`_is_node_cached()` is a module-level helper that:
+1. Extracts `CachingHook` from `hook_manager` (returns `False` if absent)
+2. Delegates to `CachingHook.is_cached(node, catalog, run_id)`
+
+`dirty_nodes` is a `set[Node]` initialized empty, passed by reference. When `_is_node_cached` returns False, all downstream nodes of that node are added to `dirty_nodes` (forces re-execution).
+
+## 8. Import-Linter Contract Changes
+
+```toml
+# In layers contract, insert "cache" between "runner" and "io":
+layers = [
+    "framework.cli",
+    "framework.session",
+    "framework.context",
+    "framework.project",
+    "runner",
+    "cache",        # NEW
+    "io",
+    "pipeline",
+    "config"
+]
+ignore_imports = [
+    "kedro.runner.task -> kedro.framework.project",
+    "kedro.framework.hooks.specs -> kedro.framework.context",
+    "kedro -> kedro.ipython",
+    "kedro.cache.hook -> kedro.framework.hooks",  # NEW: hook needs hook_impl marker
+]
+
+# In forbidden contracts, add "kedro.cache" alongside runner/io/pipeline:
+source_modules = ["kedro.runner", "kedro.io", "kedro.pipeline", "kedro.cache"]
+forbidden_modules = ["kedro.config"]
+
+source_modules = ["kedro.config"]
+forbidden_modules = ["kedro.runner", "kedro.io", "kedro.pipeline", "kedro.cache"]
+```
+
+## 9. Test Plan
+
+### Fixtures (conftest.py)
+
+- **`registry_fixture`**: Creates a temp JSON file path. Instantiates `FilesystemBackend` (not Redis — tests must be hermetic).
+- **`worker_id_fixture`**: Parametrized fixture yielding `"Worker-1"` and `"Worker-2"` via `monkeypatch.setenv("KEDRO_WORKER_ID", ...)`.
+- **`pipeline_fixture`**: Builds a 3-node pipeline (STEP_A → STEP_B → STEP_C) with:
+  - `add_numbers(a, b) -> sum` (output: parquet)
+  - `square_and_wait(sum, wait_seconds) -> squared` (output: parquet, `time.sleep(wait_seconds)`)
+  - `write_report(X) -> report` (output: TextDataset)
+- **`catalog_fixture`**: In-memory DataCatalog with MemoryDataset for params, parquet-like datasets (using MemoryDataset for unit tests, real files for integration).
+- **`caching_hook_fixture`**: Instantiated `CachingHook(registry=registry_fixture)`.
+- **`shared_utility_module`**: A large-ish Python file imported by all 3 step functions (for lazy hashing test).
+
+### Test Files
+
+| File | Coverage |
+|------|----------|
+| `test_step_cache.py` | StepCache construction, serialization, `matches()` validation logic, credential masking |
+| `test_code_hasher.py` | AST hashing correctness, mtime cache hits, transitive imports, `_hash_call_count` assertions, comment/whitespace invariance |
+| `test_registry.py` | FilesystemBackend: set/get/delete, pending sentinel handling. RedisBackend: same (requires redis fixture). |
+| `test_hook.py` | CachingHook: `before_pipeline_run` loads registry; `after_node_run` registers cache; `on_node_error` cleans up sentinel; downstream propagation via dirty_nodes |
+| `test_cache_integration.py` | **End-to-end tests per §6 of the spec:** |
+| | - §6.2 Cache Hit: run pipeline, verify 3s delay on first run, <1s on subsequent runs, cache_hits increments |
+| | - §6.3 Cache Miss: change each of environment/code_hash/input_hash/parameter_hash individually, verify re-execution (3s delay) and new StepCache entry |
+| | - §6.4 Lazy Hashing: shared utility imported by all nodes, assert `CodeHasher._hash_call_count` shows file parsed exactly once; 100-node pipeline cache overhead < 2s |
+
+### Test Assertions for §6.2 (Cache Hit)
+
+```python
+def test_cache_hit_skips_execution_and_is_fast(pipeline_fixture, registry_fixture, ...):
+    hook = CachingHook(registry=registry_fixture)
+
+    # Run 1: cold, includes 3s wait in STEP_B
+    t0 = perf_counter()
+    run_pipeline(pipeline_fixture, hook)
+    first_run_duration = perf_counter() - t0
+    assert first_run_duration >= 3.0
+
+    # Verify StepCache registered for all 3 nodes
+    assert registry_fixture.get("add_numbers") is not None
+    assert registry_fixture.get("square_and_wait") is not None
+    assert registry_fixture.get("write_report") is not None
+
+    # Run 2: warm, all cached
+    t0 = perf_counter()
+    run_pipeline(pipeline_fixture, hook)
+    second_run_duration = perf_counter() - t0
+    assert second_run_duration < 1.0
+
+    # cache_hits incremented
+    assert registry_fixture.get("square_and_wait").cache_hits == 1
+```
+
+### Test Assertions for §6.3 (Cache Miss)
+
+One test per invalidation field. Example for code change:
+```python
+def test_cache_miss_on_code_change(pipeline_fixture, registry_fixture, ...):
+    # Run 1: populate cache
+    run_pipeline(...)
+
+    # Modify STEP_B source (e.g., add a comment-free line)
+    mutate_step_b_source()
+    CodeHasher.reset()  # clear mtime cache
+
+    # Run 2: STEP_B and STEP_C (downstream) must re-execute
+    t0 = perf_counter()
+    run_pipeline(...)
+    duration = perf_counter() - t0
+    assert duration >= 3.0  # STEP_B ran again (3s wait)
+```
+
+### Test Assertions for §6.4 (Lazy Hashing)
+
+```python
+def test_lazy_code_hashing_parses_shared_module_once(pipeline_fixture, ...):
+    CodeHasher.reset()
+    # shared_utility is imported by all 3 step functions
+    run_pipeline(...)
+    # shared_utility.py parsed exactly once despite 3 nodes importing it
+    assert CodeHasher._hash_call_count_for("shared_utility.py") == 1
+
+def test_100_node_pipeline_cache_check_under_2_seconds(...):
+    pipeline_100 = build_100_node_pipeline()  # all nodes share common imports
+    CodeHasher.reset()
+    t0 = perf_counter()
+    # Only cache-check overhead (all nodes cached)
+    run_pipeline_with_cache(pipeline_100, ...)
+    assert perf_counter() - t0 < 2.0
+```
+
+## 10. Implementation Order
+
+1. `kedro/cache/step_cache.py` — Data model, no dependencies
+2. `kedro/cache/code_hasher.py` — AST hashing, stdlib only
+3. `kedro/cache/backends/filesystem_backend.py` — JSON backend
+4. `kedro/cache/backends/redis_backend.py` — Redis backend
+5. `kedro/cache/registry.py` — Registry ABC + factory (`create_registry(backend_type, ...)`)
+6. `kedro/cache/hook.py` — CachingHook (orchestrator)
+7. `kedro/cache/__init__.py` — Public exports
+8. `kedro/runner/runner.py` — Runner integration (`_is_node_cached`, dirty_nodes propagation)
+9. `pyproject.toml` — Layer contracts + redis dep
+10. Tests (all at once, verified with `pytest tests/cache/`)
+
+## 11. Verification
+
+```bash
+# Run cache-specific tests
+pytest tests/cache/ -v
+
+# Run full test suite to confirm no regressions
+make test
+
+# Lint check (import contracts enforced here)
+make lint
+```
diff --git a/CLAUDE.md b/CLAUDE.md
new file mode 100644
index 00000000..e1b2a21d
--- /dev/null
+++ b/CLAUDE.md
@@ -0,0 +1,121 @@
+# CLAUDE.md
+
+This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.
+
+## Build, Lint & Test Commands
+
+All commands are run via `make` from the repo root. The project uses `uv` as the package manager.
+
+```bash
+# Install in editable mode
+make install                      # uv pip install --system -e .
+
+# Install test dependencies
+make install-test-requirements    # uv pip install "kedro[test] @ ."
+
+# Lint (pre-commit hooks + mypy strict)
+make lint
+
+# Run unit tests (pytest, 4 parallel workers, loadfile distribution)
+make test
+
+# Run a single test file
+pytest tests/pipeline/test_node.py
+
+# Run a single test by name
+pytest tests/pipeline/test_node.py -k "test_name_here"
+
+# End-to-end / BDD tests (behave)
+make e2e-tests
+make e2e-tests-fast               # local mode, no output capture
+
+# Coverage report (HTML)
+make show-coverage
+
+# Docs
+make serve-docs                   # build and serve locally
+make build-docs                   # build only
+make linkcheck                    # validate links with lychee
+```
+
+## Architecture & Module Layout
+
+```
+kedro/
+├── config/          # Configuration loading (OmegaConfigLoader, YAML/TOML)
+├── io/              # Data catalog & dataset abstractions
+├── pipeline/        # Pipeline DAG, Node definitions
+├── runner/          # Execution engines (Sequential, Parallel, Thread)
+├── framework/
+│   ├── cli/         # Click-based CLI commands & plugin loading
+│   ├── context/     # KedroContext (holds catalog, params, config)
+│   ├── hooks/       # Pluggy-based hook specs & manager
+│   ├── project/     # Project settings (_ProjectSettings via dynaconf)
+│   └── session/     # KedroSession — orchestrates the run lifecycle
+├── ipython/         # IPython/Jupyter extension
+└── templates/       # Cookiecutter project & pipeline scaffolds
+```
+
+### Enforced Architectural Layers (import-linter)
+
+The import dependency order is strictly enforced (higher layers may import lower, not vice versa):
+
+```
+framework.cli  →  framework.session  →  framework.context  →  framework.project
+                                                                      ↓
+                                                                   runner
+                                                                      ↓
+                                                                     io
+                                                                      ↓
+                                                                  pipeline
+                                                                      ↓
+                                                                   config
+```
+
+Key contracts:
+- **`kedro.pipeline` and `kedro.io` are independent** — they must not import each other.
+- **`kedro.config` cannot import** `runner`, `io`, or `pipeline`.
+- **`runner`, `io`, `pipeline` cannot import** `config`.
+
+### Execution Flow
+
+1. `KedroSession.create()` → loads config via `OmegaConfigLoader`, creates `KedroContext`
+2. `KedroSession.run(pipeline)` → delegates to a `Runner`
+3. `Runner._run()` iterates nodes in topological order via `Task` objects
+4. Each `Task` loads inputs from `DataCatalog`, calls the node function, saves outputs
+5. Hooks fire at each lifecycle stage (`before_pipeline_run`, `before_node_run`, etc.)
+
+### Key Design Patterns
+
+- **Lazy initialization**: `_LazyDataset` defers dataset instantiation; `_ProjectPipelines` lazily loads the pipeline registry.
+- **Protocol-based extensibility**: `CatalogProtocol`, `AbstractDataset`, and the pluggy hook system allow plugins without modifying core.
+- **Transcoding**: Dataset names can encode format variants (e.g. `data@pandas` vs `data@pickle`).
+- **Immutable nodes**: `Node` instances are frozen after construction.
+
+## Test Structure
+
+Tests mirror the source layout under `tests/`. Coverage is required at 100% (with explicit omissions for templates, parallel runner, IPython extension, and color logger).
+
+```
+tests/
+├── pipeline/     # Node, Pipeline, LLMContext tests
+├── runner/       # Runner, Task, incremental-run tests
+├── io/           # DataCatalog, dataset, config-resolver tests
+├── config/       # OmegaConfigLoader tests
+├── framework/    # Session, context, hooks, CLI, project tests
+├── template/     # Template rendering tests
+└── test_import.py  # Validates public API surface
+```
+
+## Code Quality
+
+- **Linter**: `ruff` (line length 88, rules: F, W, E, I, UP, PL, T201, S, TCH, RUF)
+- **Formatter**: `ruff format`
+- **Type checking**: `mypy --strict --allow-any-generics` on the `kedro` package
+- **Pre-commit**: Includes ruff, trailing whitespace, JSON/YAML validation, detect-secrets, import-linter
+
+## PR Checklist Reminders
+
+- Update `RELEASE.md` with a description of the change.
+- Commits must be signed off (DCO compliance). Use `make sign-off` to set up the local hook.
+- Assess Kedro-Viz impact for any catalog or pipeline API changes.
diff --git a/RELEASE.md b/RELEASE.md
index 0b8ba388..1f99ca52 100644
--- a/RELEASE.md
+++ b/RELEASE.md
@@ -3,6 +3,7 @@
 * Added `@experimental` decorator to mark unstable or early-stage public APIs.
 * Added the new `support-agent-langgraph` starter. This starter contains pipelines that leverage LangGraph for agentic workflows and Langfuse or Opik for prompt management and tracing.
 * Added support for running multiple pipelines in a single Kedro session run via the `--pipelines` CLI option and `pipeline_names` argument in `KedroSession.run()` method.
+* Added pipeline step caching subsystem (`kedro.cache`) that automatically skips unchanged nodes on subsequent runs. Includes AST-based code hashing with transitive import tracking, dirty-node propagation across the DAG, and pluggable backends (filesystem JSON, Redis, S3 with ETag-based CAS).
 
 ## Experimental features
 * Added experimental `llm_context_node` and `LLMContextNode` for assembling LLMs, prompts, and tools into a runtime `LLMContext` within Kedro pipelines.
diff --git a/data/test.pkl b/data/test.pkl
new file mode 100644
index 00000000..bf05da2e
Binary files /dev/null and b/data/test.pkl differ
diff --git a/kedro/cache/__init__.py b/kedro/cache/__init__.py
new file mode 100644
index 00000000..60f90546
--- /dev/null
+++ b/kedro/cache/__init__.py
@@ -0,0 +1,26 @@
+"""``kedro.cache`` provides pipeline-level node caching.
+
+Public API surface:
+
+- :class:`StepCache` — immutable snapshot of a node's execution context.
+- :class:`CodeHasher` — AST-based, memoized function hasher.
+- :class:`CacheRegistry` — abstract backend interface.
+- :func:`create_registry` — factory for concrete backends.
+- :class:`CachingHook` — pluggy hook orchestrating cache check & registration.
+- :func:`cache_transaction` — context manager for transactional cache writes.
+"""
+
+from kedro.cache.code_hasher import CodeHasher, SharedHashCache
+from kedro.cache.hook import CachingHook, cache_transaction
+from kedro.cache.registry import CacheRegistry, create_registry
+from kedro.cache.step_cache import StepCache
+
+__all__ = [
+    "CacheRegistry",
+    "CachingHook",
+    "CodeHasher",
+    "SharedHashCache",
+    "StepCache",
+    "cache_transaction",
+    "create_registry",
+]
diff --git a/kedro/cache/_locks.py b/kedro/cache/_locks.py
new file mode 100644
index 00000000..a76085ac
--- /dev/null
+++ b/kedro/cache/_locks.py
@@ -0,0 +1,44 @@
+"""Cross-platform exclusive file lock used by cache backends and SharedHashCache."""
+
+from __future__ import annotations
+
+import os
+import sys
+from contextlib import contextmanager
+from typing import TYPE_CHECKING
+
+if TYPE_CHECKING:
+    from collections.abc import Generator
+
+
+@contextmanager
+def file_lock(lock_path: str) -> Generator[None, None, None]:
+    """Acquire an exclusive advisory file lock at *lock_path*.
+
+    The lock is automatically released when the context exits or when the
+    process crashes - the OS reclaims all locks on fd close.
+
+    On POSIX (Linux / macOS) this uses ``fcntl.flock``.  On Windows it falls
+    back to ``msvcrt.locking``.  Both flavours are blocking: the caller will
+    wait until the lock is available.
+    """
+    fd = os.open(lock_path, os.O_CREAT | os.O_WRONLY, 0o644)
+    try:
+        if sys.platform == "win32":
+            import msvcrt  # noqa: PLC0415
+
+            msvcrt.locking(fd, msvcrt.LK_LOCK, 1)
+            try:
+                yield
+            finally:
+                msvcrt.locking(fd, msvcrt.LK_UNLCK, 1)
+        else:
+            import fcntl  # noqa: PLC0415
+
+            fcntl.flock(fd, fcntl.LOCK_EX)
+            try:
+                yield
+            finally:
+                fcntl.flock(fd, fcntl.LOCK_UN)
+    finally:
+        os.close(fd)
diff --git a/kedro/cache/backends/__init__.py b/kedro/cache/backends/__init__.py
new file mode 100644
index 00000000..95a32d51
--- /dev/null
+++ b/kedro/cache/backends/__init__.py
@@ -0,0 +1,7 @@
+"""Cache backend implementations."""
+
+from kedro.cache.backends.filesystem_backend import FilesystemBackend
+from kedro.cache.backends.redis_backend import RedisBackend
+from kedro.cache.backends.s3_backend import S3Backend
+
+__all__ = ["FilesystemBackend", "RedisBackend", "S3Backend"]
diff --git a/kedro/cache/backends/filesystem_backend.py b/kedro/cache/backends/filesystem_backend.py
new file mode 100644
index 00000000..1531ac24
--- /dev/null
+++ b/kedro/cache/backends/filesystem_backend.py
@@ -0,0 +1,120 @@
+"""JSON file-backed cache registry with exclusive file locking."""
+
+from __future__ import annotations
+
+import json
+import logging
+import os
+import tempfile
+from typing import TYPE_CHECKING, Any
+
+if TYPE_CHECKING:
+    from collections.abc import Callable
+
+from kedro.cache._locks import file_lock
+from kedro.cache.registry import CacheRegistry
+from kedro.cache.step_cache import StepCache
+
+logger = logging.getLogger(__name__)
+
+
+class FilesystemBackend(CacheRegistry):
+    """Stores cache entries as a single JSON file on disk.
+
+    Concurrent access is serialised by an exclusive file lock (``fcntl.flock``
+    on POSIX, ``msvcrt.locking`` on Windows).  Every public method acquires the
+    lock before reading or writing, so the read-modify-write cycle is atomic
+    with respect to other processes using the same *path*.
+
+    Writes additionally use an atomic-rename pattern: data goes to a temp file
+    first, then ``os.replace`` swaps it in, so a reader never sees a
+    half-written JSON file even if the writer crashes mid-write.
+    """
+
+    def __init__(self, path: str = ".kedro_cache.json") -> None:
+        self._path = os.path.abspath(path)
+        self._lock_path = self._path + ".lock"
+
+    # ------------------------------------------------------------------
+    # Internal helpers (must be called WITH the lock held)
+    # ------------------------------------------------------------------
+
+    def _load_store(self) -> dict[str, Any]:
+        if not os.path.isfile(self._path):
+            return {}
+        try:
+            with open(self._path, encoding="utf-8") as f:
+                return json.load(f)  # type: ignore[no-any-return]
+        except (json.JSONDecodeError, OSError):
+            logger.warning("Cache file %s is corrupt; starting fresh.", self._path)
+            return {}
+
+    def _save_store(self, store: dict[str, Any]) -> None:
+        directory = os.path.dirname(self._path) or "."
+        fd, tmp = tempfile.mkstemp(dir=directory, suffix=".tmp")
+        try:
+            with os.fdopen(fd, "w", encoding="utf-8") as f:
+                json.dump(store, f, indent=2, default=str)
+            os.replace(tmp, self._path)
+        except Exception:
+            if os.path.exists(tmp):
+                os.remove(tmp)
+            raise
+
+    # ------------------------------------------------------------------
+    # CacheRegistry interface
+    # ------------------------------------------------------------------
+
+    def get(self, key: str) -> StepCache | None:
+        with file_lock(self._lock_path):
+            store = self._load_store()
+            raw = store.get(key)
+            if raw is None:
+                return None
+            if key.endswith(".__pending__"):
+                return None
+            try:
+                return StepCache.from_dict(raw)
+            except (TypeError, KeyError):
+                logger.warning("Corrupt cache entry for key %r; ignoring.", key)
+                return None
+
+    def set(self, key: str, step_cache: StepCache) -> None:
+        with file_lock(self._lock_path):
+            store = self._load_store()
+            store[key] = step_cache.to_dict()
+            self._save_store(store)
+
+    def delete(self, key: str) -> None:
+        with file_lock(self._lock_path):
+            store = self._load_store()
+            store.pop(key, None)
+            self._save_store(store)
+
+    def atomic_update(
+        self,
+        key: str,
+        updater: Callable[[StepCache | None], StepCache | None],
+    ) -> None:
+        """Read, transform, and write a cache entry under a single lock acquisition.
+
+        *updater* receives the current ``StepCache`` (or ``None``) and returns
+        the desired new value (or ``None`` to delete the entry).  The entire
+        read-modify-write is atomic with respect to other processes.
+        """
+        with file_lock(self._lock_path):
+            store = self._load_store()
+            raw = store.get(key)
+            current: StepCache | None = None
+            if raw is not None and not key.endswith(".__pending__"):
+                try:
+                    current = StepCache.from_dict(raw)
+                except (TypeError, KeyError):
+                    current = None
+
+            updated = updater(current)
+            if updated is not None:
+                store[key] = updated.to_dict()
+            elif key in store:
+                del store[key]
+            self._save_store(store)
diff --git a/kedro/cache/backends/redis_backend.py b/kedro/cache/backends/redis_backend.py
new file mode 100644
index 00000000..5a493e4c
--- /dev/null
+++ b/kedro/cache/backends/redis_backend.py
@@ -0,0 +1,188 @@
+"""Redis-backed persistent cache registry with Lua-script transaction safety."""
+
+from __future__ import annotations
+
+import logging
+from typing import TYPE_CHECKING
+
+from kedro.cache.registry import CacheRegistry
+from kedro.cache.step_cache import StepCache
+
+if TYPE_CHECKING:
+    from collections.abc import Callable
+
+logger = logging.getLogger(__name__)
+
+_HASH_KEY = "kedro:step_cache"
+
+# ---------------------------------------------------------------------------
+# Lua scripts — executed atomically on the Redis server so that compound
+# read-modify-write operations cannot be interleaved by concurrent clients.
+# ---------------------------------------------------------------------------
+
+# Atomically increment the ``cache_hits`` field inside a JSON-encoded hash
+# field.  Returns 1 on success, 0 if the field does not exist.
+_LUA_INCREMENT_HITS = """
+local raw = redis.call('HGET', KEYS[1], KEYS[2])
+if not raw then
+    return 0
+end
+local data = cjson.decode(raw)
+data.cache_hits = (data.cache_hits or 0) + 1
+redis.call('HSET', KEYS[1], KEYS[2], cjson.encode(data))
+return 1
+"""
+
+# Optimistic compare-and-swap for a single hash field.
+# KEYS[1] = hash key, KEYS[2] = field name
+# ARGV[1] = expected current raw value (empty string means "don't compare")
+# ARGV[2] = new raw value (empty string means "delete the field")
+# Returns 1 on success, 0 on version mismatch.
+_LUA_COMPARE_AND_SWAP = """
+local current = redis.call('HGET', KEYS[1], KEYS[2])
+local expected = ARGV[1]
+if expected ~= "" and tostring(current) ~= expected then
+    return 0
+end
+local new_val = ARGV[2]
+if new_val == "" then
+    redis.call('HDEL', KEYS[1], KEYS[2])
+else
+    redis.call('HSET', KEYS[1], KEYS[2], new_val)
+end
+return 1
+"""
+
+
+class RedisBackend(CacheRegistry):
+    """Cache registry backed by a Redis hash with server-side transaction safety.
+
+    Simple ``get`` / ``set`` / ``delete`` map directly to ``HGET`` / ``HSET`` /
+    ``HDEL``, which are individually atomic.
+
+    Compound operations (``atomic_update``, ``atomic_increment_cache_hits``) use
+    pre-registered Lua scripts so that read-modify-write cycles execute in a
+    single atomic server round-trip.  ``atomic_update`` additionally wraps the
+    Lua call in an optimistic retry loop to handle the (rare) case where the
+    script itself is evicted between registration and invocation.
+
+    Args:
+        host: Redis host. Defaults to ``"localhost"``.
+        port: Redis port. Defaults to ``6379``.
+        db: Redis database index. Defaults to ``0``.
+        password: Optional Redis password.
+        hash_key: Redis hash key. Defaults to ``"kedro:step_cache"``.
+        max_retries: Maximum CAS retry attempts for ``atomic_update``.
+    """
+
+    def __init__(  # noqa: PLR0913
+        self,
+        host: str = "localhost",
+        port: int = 6379,
+        db: int = 0,
+        password: str | None = None,
+        hash_key: str = _HASH_KEY,
+        max_retries: int = 10,
+    ) -> None:
+        import redis  # noqa: PLC0415
+
+        self._client = redis.Redis(
+            host=host,
+            port=port,
+            db=db,
+            password=password,
+            decode_responses=True,
+        )
+        self._hash_key = hash_key
+        self._max_retries = max_retries
+
+        # Pre-register Lua scripts — avoids re-sending the script body on
+        # every call; Redis caches by SHA and the client falls back to EVAL
+        # transparently if the server has evicted it.
+        self._increment_hits_script = self._client.register_script(_LUA_INCREMENT_HITS)
+        self._cas_script = self._client.register_script(_LUA_COMPARE_AND_SWAP)
+
+    # ------------------------------------------------------------------
+    # CacheRegistry interface — simple atomic ops
+    # ------------------------------------------------------------------
+
+    def get(self, key: str) -> StepCache | None:
+        if key.endswith(".__pending__"):
+            return None
+        raw = self._client.hget(self._hash_key, key)
+        if raw is None:
+            return None
+        try:
+            return StepCache.from_json(raw)
+        except (TypeError, KeyError, Exception):
+            logger.warning("Corrupt Redis cache entry for key %r; ignoring.", key)
+            return None
+
+    def set(self, key: str, step_cache: StepCache) -> None:
+        self._client.hset(self._hash_key, key, step_cache.to_json())
+
+    def delete(self, key: str) -> None:
+        self._client.hdel(self._hash_key, key)
+
+    def set_pending(self, sentinel: str) -> None:
+        """Lightweight sentinel — an empty JSON object, ignored by ``get``."""
+        self._client.hset(self._hash_key, sentinel, "{}")
+
+    # ------------------------------------------------------------------
+    # Transaction-safe compound operations
+    # ------------------------------------------------------------------
+
+    def atomic_update(
+        self,
+        key: str,
+        updater: Callable[[StepCache | None], StepCache | None],
+    ) -> None:
+        """Read, transform, and write under an optimistic CAS loop.
+
+        The loop reads the current raw value, applies *updater* in Python,
+        then issues a compare-and-swap Lua script that only commits if the
+        field has not been modified since the read.  On conflict the loop
+        retries up to ``max_retries`` times.
+        """
+        for attempt in range(self._max_retries):
+            raw = self._client.hget(self._hash_key, key)
+
+            current: StepCache | None = None
+            if raw is not None and not key.endswith(".__pending__"):
+                try:
+                    current = StepCache.from_json(raw)
+                except (TypeError, KeyError, Exception):
+                    current = None
+
+            updated = updater(current)
+            new_json = updated.to_json() if updated is not None else ""
+            expected_json = raw if raw is not None else ""
+
+            result = self._cas_script(
+                keys=[self._hash_key, key],
+                args=[expected_json, new_json],
+            )
+            if result == 1:
+                return  # committed successfully
+
+            logger.debug(
+                "CAS conflict on key %r (attempt %d/%d), retrying.",
+                key,
+                attempt + 1,
+                self._max_retries,
+            )
+
+        logger.warning(
+            "Failed to atomically update key %r after %d retries.",
+            key,
+            self._max_retries,
+        )
+
+    def atomic_increment_cache_hits(self, key: str) -> bool:
+        """Atomically increment ``cache_hits`` via a server-side Lua script.
+
+        Returns ``True`` if the field existed and was updated, ``False`` if
+        the key was not found.
+        """
+        result = self._increment_hits_script(keys=[self._hash_key, key])
+        return result == 1
diff --git a/kedro/cache/backends/s3_backend.py b/kedro/cache/backends/s3_backend.py
new file mode 100644
index 00000000..4c7d6460
--- /dev/null
+++ b/kedro/cache/backends/s3_backend.py
@@ -0,0 +1,206 @@
+"""S3-backed cache registry with ETag-based optimistic concurrency control."""
+
+from __future__ import annotations
+
+import json
+import logging
+from typing import TYPE_CHECKING, Any
+
+if TYPE_CHECKING:
+    from collections.abc import Callable
+
+from kedro.cache.registry import CacheRegistry
+from kedro.cache.step_cache import StepCache
+
+logger = logging.getLogger(__name__)
+
+
+class S3Backend(CacheRegistry):
+    """Cache registry backed by a single JSON object in Amazon S3.
+
+    The entire cache store is a single JSON object.  Concurrent writes are
+    resolved via S3's conditional-PUT mechanism: each write reads the object
+    and its ``ETag``, modifies the payload in memory, then issues a
+    ``PutObject`` with ``IfMatch=<etag>``.  If another writer modified the
+    object in the meantime, S3 returns ``412 Precondition Failed`` and the
+    operation is retried (up to ``max_retries`` times).
+
+    This is the S3-native equivalent of Redis CAS or filesystem file locks.
+
+    Args:
+        bucket: S3 bucket name.
+        key: Object key (path within bucket).
+            Defaults to ``"kedro_cache.json"``.
+        region: AWS region name. Defaults to ``"us-east-1"``.
+        endpoint_url: Optional custom endpoint URL (MinIO, LocalStack, etc.).
+        max_retries: Maximum conditional-PUT retry attempts.
+            Defaults to ``10``.
+        aws_access_key_id: Optional explicit AWS access key.
+        aws_secret_access_key: Optional explicit AWS secret key.
+    """
+
+    def __init__(  # noqa: PLR0913
+        self,
+        bucket: str,
+        key: str = "kedro_cache.json",
+        region: str = "us-east-1",
+        endpoint_url: str | None = None,
+        max_retries: int = 10,
+        aws_access_key_id: str | None = None,
+        aws_secret_access_key: str | None = None,
+    ) -> None:
+        import boto3  # noqa: PLC0415
+
+        session_kwargs: dict[str, Any] = {"region_name": region}
+        if aws_access_key_id:
+            session_kwargs["aws_access_key_id"] = aws_access_key_id
+        if aws_secret_access_key:
+            session_kwargs["aws_secret_access_key"] = aws_secret_access_key
+
+        client_kwargs: dict[str, Any] = {}
+        if endpoint_url:
+            client_kwargs["endpoint_url"] = endpoint_url
+
+        session = boto3.Session(**session_kwargs)
+        self._client = session.client("s3", **client_kwargs)
+        self._bucket = bucket
+        self._key = key
+        self._max_retries = max_retries
+
+    # ------------------------------------------------------------------
+    # Internal helpers
+    # ------------------------------------------------------------------
+
+    def _get_object(self) -> tuple[dict[str, Any], str | None]:
+        """Fetch the cache store and its ETag.
+
+        Returns:
+            Tuple of ``(store_dict, etag)``.  *etag* is ``None`` when the
+            object does not yet exist in S3.
+        """
+        try:
+            response = self._client.get_object(Bucket=self._bucket, Key=self._key)
+            body = response["Body"].read().decode("utf-8")
+            etag = response.get("ETag", "")
+            return json.loads(body), etag
+        except Exception as exc:
+            # boto3 raises ClientError with Code == "NoSuchKey"
+            error_code = ""
+            if hasattr(exc, "response") and isinstance(exc.response, dict):
+                error_code = exc.response.get("Error", {}).get("Code", "")
+            if error_code == "NoSuchKey" or "NoSuchKey" in type(exc).__name__:
+                return {}, None
+            # Truly unexpected error
+            logger.warning(
+                "S3 cache read failed (%s/%s): %s; starting fresh.",
+                self._bucket,
+                self._key,
+                exc,
+            )
+            return {}, None
+
+    def _put_object(self, store: dict[str, Any], etag: str | None) -> bool:
+        """Write *store* back to S3, conditionally on *etag* if provided.
+
+        Returns:
+            ``True`` on success, ``False`` on ``412 Precondition Failed``
+            (ETag mismatch — another writer won the race).
+        """
+        body = json.dumps(store, indent=2, default=str).encode("utf-8")
+        put_kwargs: dict[str, Any] = {
+            "Bucket": self._bucket,
+            "Key": self._key,
+            "Body": body,
+            "ContentType": "application/json",
+        }
+        if etag is not None:
+            put_kwargs["IfMatch"] = etag
+
+        try:
+            self._client.put_object(**put_kwargs)
+            return True
+        except Exception as exc:
+            # Duck-type the error: boto3 ClientError carries
+            # .response["Error"]["Code"] == "PreconditionFailed"
+            error_code = ""
+            if hasattr(exc, "response") and isinstance(exc.response, dict):
+                error_code = exc.response.get("Error", {}).get("Code", "")
+            if error_code == "PreconditionFailed":
+                return False
+            raise
+
+    # ------------------------------------------------------------------
+    # CacheRegistry interface
+    # ------------------------------------------------------------------
+
+    def get(self, key: str) -> StepCache | None:
+        if key.endswith(".__pending__"):
+            return None
+        store, _ = self._get_object()
+        raw = store.get(key)
+        if raw is None:
+            return None
+        try:
+            return StepCache.from_dict(raw)
+        except (TypeError, KeyError):
+            logger.warning("Corrupt S3 cache entry for key %r; ignoring.", key)
+            return None
+
+    def set(self, key: str, step_cache: StepCache) -> None:
+        store, etag = self._get_object()
+        store[key] = step_cache.to_dict()
+        self._put_object(store, etag)
+
+    def delete(self, key: str) -> None:
+        store, etag = self._get_object()
+        store.pop(key, None)
+        self._put_object(store, etag)
+
+    def set_pending(self, sentinel: str) -> None:
+        """Write a lightweight sentinel entry."""
+        store, etag = self._get_object()
+        store[sentinel] = {"step_id": sentinel}
+        self._put_object(store, etag)
+
+    def atomic_update(
+        self,
+        key: str,
+        updater: Callable[[StepCache | None], StepCache | None],
+    ) -> None:
+        """ETag-based optimistic CAS loop.
+
+        Reads the object and its ETag, applies *updater* in memory, then
+        issues a conditional ``PutObject``.  On ``412 Precondition Failed``
+        (another writer modified the object), the loop retries from scratch.
+        """
+        for attempt in range(self._max_retries):
+            store, etag = self._get_object()
+            raw = store.get(key)
+            current: StepCache | None = None
+            if raw is not None and not key.endswith(".__pending__"):
+                try:
+                    current = StepCache.from_dict(raw)
+                except (TypeError, KeyError):
+                    current = None
+
+            updated = updater(current)
+            if updated is not None:
+                store[key] = updated.to_dict()
+            elif key in store:
+                del store[key]
+
+            if self._put_object(store, etag):
+                return  # committed successfully
+
+            logger.debug(
+                "S3 ETag conflict on key %r (attempt %d/%d), retrying.",
+                key,
+                attempt + 1,
+                self._max_retries,
+            )
+
+        logger.warning(
+            "Failed to atomically update key %r after %d retries.",
+            key,
+            self._max_retries,
+        )
diff --git a/kedro/cache/code_hasher.py b/kedro/cache/code_hasher.py
new file mode 100644
index 00000000..fc69ea03
--- /dev/null
+++ b/kedro/cache/code_hasher.py
@@ -0,0 +1,450 @@
+"""AST-based code hashing with two-level memoization and transitive imports.
+
+Architecture
+------------
+``CodeHasher`` maintains two cache levels:
+
+* **L1 — in-process dict** (``_file_cache``).  Keyed by ``(filepath, mtime)``.
+  Zero IPC cost; invalidated automatically when ``mtime`` changes.
+* **L2 — shared file cache** (``SharedHashCache``).  A JSON file protected by
+  an exclusive file lock, shared across worker processes spawned by
+  ``ProcessPoolExecutor``.  Configured once via ``configure_shared_cache()``.
+
+``SharedHashCache.get_or_compute`` uses a *check - lock - check* pattern so
+that the file lock is **never held** during the potentially-recursive AST
+parsing.  This prevents deadlock when a transitive import triggers another
+``_hash_file`` call that also needs the lock.
+"""
+
+from __future__ import annotations
+
+import ast
+import hashlib
+import inspect
+import json
+import os
+import tempfile
+from typing import TYPE_CHECKING, ClassVar
+
+if TYPE_CHECKING:
+    from collections.abc import Callable
+
+from kedro.cache._locks import file_lock
+
+# ---------------------------------------------------------------------------
+# AST normalisation helpers
+# ---------------------------------------------------------------------------
+
+
+class _ASTNormalizer(ast.NodeVisitor):
+    """Walks an AST and collects a normalised, whitespace/comment-insensitive
+    representation suitable for hashing.
+
+    Only collects top-level statement nodes — comments and docstrings that
+    don't affect execution are stripped.
+    """
+
+    def __init__(self) -> None:
+        self.chunks: list[str] = []
+
+    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def visit_ClassDef(self, node: ast.ClassDef) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def visit_Assign(self, node: ast.Assign) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def visit_Import(self, node: ast.Import) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def visit_Return(self, node: ast.Return) -> None:
+        self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+    def generic_visit(self, node: ast.AST) -> None:
+        if isinstance(node, ast.Module):
+            for child in ast.iter_child_nodes(node):
+                self.visit(child)
+        elif isinstance(node, (ast.Expr, ast.AugAssign, ast.AnnAssign)):
+            self.chunks.append(ast.dump(node, annotate_fields=True, indent=None))
+
+
+def _collect_local_imports(tree: ast.Module) -> list[str]:
+    """Extract module names from Import and ImportFrom statements."""
+    modules: list[str] = []
+    for node in ast.walk(tree):
+        if isinstance(node, ast.Import):
+            for alias in node.names:
+                modules.append(alias.name)
+        elif isinstance(node, ast.ImportFrom) and node.module:
+            modules.append(node.module)
+    return modules
+
+
+def _resolve_module_to_file(module_name: str, project_root: str) -> str | None:
+    """Attempt to resolve a dotted module name to a .py file under project_root."""
+    parts = module_name.split(".")
+    # Try as package (directory with __init__.py)
+    package_path = os.path.join(project_root, *parts, "__init__.py")
+    if os.path.isfile(package_path):
+        return package_path
+    # Try as module file
+    if len(parts) > 1:
+        module_path = os.path.join(project_root, *parts[:-1], f"{parts[-1]}.py")
+    else:
+        module_path = os.path.join(project_root, f"{parts[0]}.py")
+    if os.path.isfile(module_path):
+        return module_path
+    # Try as single file
+    single_path = os.path.join(project_root, *parts) + ".py"
+    if os.path.isfile(single_path):
+        return single_path
+    return None
+
+
+# ---------------------------------------------------------------------------
+# Shared cross-process hash cache
+# ---------------------------------------------------------------------------
+
+
+class SharedHashCache:
+    """File-backed hash cache shared across worker processes.
+
+    A single JSON file stores ``{shared_key: sha256_hex}`` mappings.  Every
+    read or write acquires an exclusive file lock so concurrent workers never
+    corrupt the store.
+
+    ``get_or_compute`` implements the *check - lock - check - compute - lock -
+    write* pattern:
+
+    1. Acquire lock, read store.
+    2. On hit, return immediately (lock released).
+    3. On miss, **release** the lock before computing (AST parsing is
+       expensive and may recurse into other ``_hash_file`` calls that also
+       need the lock).
+    4. Re-acquire lock and write the result.
+
+    Two processes may redundantly compute the same hash in step 3.  That is
+    harmless: hashes are deterministic, so the last writer simply overwrites
+    with an identical value.
+    """
+
+    def __init__(self, cache_path: str) -> None:
+        self._path = os.path.abspath(cache_path)
+        self._lock_path = self._path + ".lock"
+
+    # ------------------------------------------------------------------
+
+    def _load(self) -> dict[str, str]:
+        if not os.path.isfile(self._path):
+            return {}
+        try:
+            with open(self._path, encoding="utf-8") as f:
+                return json.load(f)  # type: ignore[no-any-return]
+        except (json.JSONDecodeError, OSError):
+            return {}
+
+    def _save(self, store: dict[str, str]) -> None:
+        directory = os.path.dirname(self._path) or "."
+        fd, tmp = tempfile.mkstemp(dir=directory, suffix=".tmp")
+        try:
+            with os.fdopen(fd, "w", encoding="utf-8") as f:
+                json.dump(store, f)
+            os.replace(tmp, self._path)
+        except Exception:
+            if os.path.exists(tmp):
+                os.remove(tmp)
+            raise
+
+    def get(self, key: str) -> str | None:
+        """Read a single entry under lock."""
+        with file_lock(self._lock_path):
+            return self._load().get(key)
+
+    def set(self, key: str, value: str) -> None:
+        """Write a single entry under lock."""
+        with file_lock(self._lock_path):
+            store = self._load()
+            store[key] = value
+            self._save(store)
+
+    def get_or_compute(self, key: str, compute_fn: Callable[[], str]) -> str:
+        """Check - lock - check pattern.
+
+        The lock is released **before** ``compute_fn`` runs so that recursive
+        ``_hash_file`` calls (for transitive imports) can independently acquire
+        and release the lock without deadlocking.
+        """
+        # Step 1: read under lock
+        with file_lock(self._lock_path):
+            store = self._load()
+            cached = store.get(key)
+            if cached is not None:
+                return cached
+
+        # Step 2: compute WITHOUT holding the lock — may recurse
+        value = compute_fn()
+
+        # Step 3: write under lock (idempotent — value is deterministic)
+        with file_lock(self._lock_path):
+            store = self._load()
+            store[key] = value
+            self._save(store)
+
+        return value
+
+    def clear(self) -> None:
+        """Remove the shared cache file (under lock)."""
+        with file_lock(self._lock_path):
+            if os.path.isfile(self._path):
+                os.remove(self._path)
+
+
+# ---------------------------------------------------------------------------
+# CodeHasher
+# ---------------------------------------------------------------------------
+
+
+class CodeHasher:
+    """Singleton-per-process AST-based code hasher with two-level caching.
+
+    * **L1** — ``_file_cache``: in-process dict, keyed by ``(filepath, mtime)``.
+    * **L2** — ``_shared_cache``: optional ``SharedHashCache`` shared across
+      worker processes, activated by ``configure_shared_cache()``.
+
+    When L2 is active, ``_hash_file`` delegates to
+    ``SharedHashCache.get_or_compute`` which holds the file lock only for
+    cache reads/writes, never during the recursive AST parse.
+    """
+
+    _file_cache: ClassVar[dict[tuple[str, float], str]] = {}
+    _shared_cache: ClassVar[SharedHashCache | None] = None
+    _hash_call_count: ClassVar[int] = 0
+    _file_call_counts: ClassVar[dict[str, int]] = {}
+
+    @classmethod
+    def configure_shared_cache(cls, cache_path: str) -> None:
+        """Activate cross-process shared hash cache at *cache_path*."""
+        cls._shared_cache = SharedHashCache(cache_path)
+
+    @classmethod
+    def hash_function(cls, func: Callable) -> str:
+        """Entry point: return SHA-256 hex digest of *func* and its transitive imports.
+
+        Hashes the specific function's AST node (not the whole file) combined
+        with the file-level transitive import hashes so that two functions in
+        the same file produce distinct digests.
+        """
+        try:
+            source_file = inspect.getfile(func)
+        except (TypeError, OSError):
+            # Built-in or dynamically defined function — hash the repr
+            return hashlib.sha256(repr(func).encode("utf-8")).hexdigest()
+
+        source_file = os.path.abspath(source_file)
+        project_root = cls._detect_project_root(source_file)
+
+        # Extract the function's own AST node for a per-function digest
+        func_node_repr = cls._extract_function_node_repr(func, source_file)
+
+        # Get file-level import hashes (transitive dependencies)
+        file_hash = cls._hash_file(source_file, project_root)
+
+        combined = func_node_repr + "\n" + file_hash
+        return hashlib.sha256(combined.encode("utf-8")).hexdigest()
+
+    @classmethod
+    def _extract_function_node_repr(cls, func: Callable, source_file: str) -> str:
+        """Return the AST dump of the specific function node, or fallback to repr."""
+        func_name = getattr(func, "__name__", None)
+        if func_name is None:
+            return repr(func)
+        try:
+            with open(source_file, encoding="utf-8") as f:
+                tree = ast.parse(f.read())
+        except (OSError, SyntaxError):
+            return repr(func)
+
+        for node in ast.walk(tree):
+            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
+                if node.name == func_name:
+                    return ast.dump(node, annotate_fields=True, indent=None)
+        # Fallback: lambda or nested function — use repr
+        return repr(func)
+
+    @classmethod
+    def _detect_project_root(cls, filepath: str) -> str:
+        """Walk up from *filepath* to find the nearest directory containing
+        a pyproject.toml or setup.py — used to bound transitive import resolution.
+        """
+        current = os.path.dirname(filepath)
+        for _ in range(20):  # safety limit
+            if os.path.isfile(os.path.join(current, "pyproject.toml")):
+                return current
+            if os.path.isfile(os.path.join(current, "setup.py")):
+                return current
+            parent = os.path.dirname(current)
+            if parent == current:
+                break
+            current = parent
+        # Fallback: directory of the source file
+        return os.path.dirname(filepath)
+
+    @classmethod
+    def _hash_file(
+        cls,
+        filepath: str,
+        project_root: str,
+        _seen: set[str] | None = None,
+    ) -> str:
+        """Hash a single file with two-level caching and recursive import following."""
+        if _seen is None:
+            _seen = set()
+        if filepath in _seen:
+            return ""
+        _seen.add(filepath)
+
+        try:
+            mtime = os.path.getmtime(filepath)
+        except OSError:
+            return hashlib.sha256(filepath.encode("utf-8")).hexdigest()
+
+        # L1: in-process cache
+        l1_key = (filepath, mtime)
+        if l1_key in cls._file_cache:
+            return cls._file_cache[l1_key]
+
+        # L2: shared cross-process cache (if configured)
+        shared_key = f"{filepath}:{mtime}"
+        if cls._shared_cache is not None:
+
+            def _compute() -> str:
+                return cls._compute_file_hash(filepath, project_root, _seen)
+
+            digest = cls._shared_cache.get_or_compute(shared_key, _compute)
+            cls._file_cache[l1_key] = digest
+            return digest
+
+        # No shared cache — compute directly
+        digest = cls._compute_file_hash(filepath, project_root, _seen)
+        cls._file_cache[l1_key] = digest
+        return digest
+
+    @classmethod
+    def _compute_file_hash(
+        cls,
+        filepath: str,
+        project_root: str,
+        _seen: set[str],
+    ) -> str:
+        """Parse, normalise AST, follow project-local imports, return SHA-256."""
+        cls._hash_call_count += 1
+        basename = os.path.basename(filepath)
+        cls._file_call_counts[basename] = cls._file_call_counts.get(basename, 0) + 1
+
+        try:
+            with open(filepath, encoding="utf-8") as f:
+                source = f.read()
+            tree = ast.parse(source)
+        except (OSError, SyntaxError):
+            return hashlib.sha256(filepath.encode("utf-8")).hexdigest()
+
+        # Normalise AST
+        normalizer = _ASTNormalizer()
+        normalizer.visit(tree)
+        own_repr = "\n".join(normalizer.chunks)
+
+        # Follow project-local imports recursively
+        imported_hashes: list[str] = []
+        for module_name in _collect_local_imports(tree):
+            resolved = _resolve_module_to_file(module_name, project_root)
+            if resolved is not None and os.path.abspath(resolved).startswith(
+                project_root
+            ):
+                sub_hash = cls._hash_file(
+                    os.path.abspath(resolved), project_root, _seen
+                )
+                if sub_hash:
+                    imported_hashes.append(sub_hash)
+
+        # Combine own AST repr + sorted transitive hashes
+        combined = own_repr + "\n" + "\n".join(sorted(imported_hashes))
+        return hashlib.sha256(combined.encode("utf-8")).hexdigest()
+
+    @classmethod
+    def file_hashes_for(cls, func: Callable) -> dict[str, str]:
+        """Return ``{basename: file_hash}`` for *func* and its transitive imports.
+
+        Reuses the same L1/L2 caches as ``hash_function`` so no extra parsing
+        occurs when called immediately after ``hash_function``.
+        """
+        try:
+            source_file = inspect.getfile(func)
+        except (TypeError, OSError):
+            return {}
+
+        source_file = os.path.abspath(source_file)
+        project_root = cls._detect_project_root(source_file)
+
+        result: dict[str, str] = {}
+        cls._collect_transitive_hashes(source_file, project_root, result)
+        return result
+
+    @classmethod
+    def _collect_transitive_hashes(
+        cls,
+        filepath: str,
+        project_root: str,
+        result: dict[str, str],
+        _seen: set[str] | None = None,
+    ) -> None:
+        """Walk the import tree and populate *result* with per-file hashes."""
+        if _seen is None:
+            _seen = set()
+        if filepath in _seen:
+            return
+        _seen.add(filepath)
+
+        # Pass a copy that excludes the current file so _hash_file's own
+        # cycle guard doesn't short-circuit on the file we're asking about.
+        hash_seen = _seen.copy()
+        hash_seen.discard(filepath)
+        file_hash = cls._hash_file(filepath, project_root, hash_seen)
+        result[os.path.basename(filepath)] = file_hash
+
+        try:
+            with open(filepath, encoding="utf-8") as f:
+                tree = ast.parse(f.read())
+        except (OSError, SyntaxError):
+            return
+
+        for module_name in _collect_local_imports(tree):
+            resolved = _resolve_module_to_file(module_name, project_root)
+            if resolved is not None:
+                resolved = os.path.abspath(resolved)
+                if resolved.startswith(project_root):
+                    cls._collect_transitive_hashes(
+                        resolved, project_root, result, _seen
+                    )
+
+    @classmethod
+    def reset(cls) -> None:
+        """Clear all cached state including the shared cache file."""
+        cls._file_cache.clear()
+        cls._hash_call_count = 0
+        cls._file_call_counts.clear()
+        if cls._shared_cache is not None:
+            cls._shared_cache.clear()
+            cls._shared_cache = None
+
+    @classmethod
+    def hash_call_count_for(cls, filename: str) -> int:
+        """Return how many times *filename* was actually parsed (cache misses)."""
+        return cls._file_call_counts.get(filename, 0)
diff --git a/kedro/cache/hook.py b/kedro/cache/hook.py
new file mode 100644
index 00000000..c330ba92
--- /dev/null
+++ b/kedro/cache/hook.py
@@ -0,0 +1,356 @@
+"""CachingHook — pluggy-based hook that orchestrates the step-cache lifecycle."""
+
+from __future__ import annotations
+
+import logging
+import os
+from contextlib import contextmanager
+from datetime import datetime, timezone
+from typing import TYPE_CHECKING, Any
+
+from kedro.cache.code_hasher import CodeHasher
+from kedro.cache.registry import CacheRegistry  # noqa: TC001
+from kedro.cache.step_cache import (
+    StepCache,
+    _mask_cli_flags,
+    compute_input_hash,
+    compute_parameter_hash,
+)
+from kedro.framework.hooks import hook_impl
+
+if TYPE_CHECKING:
+    from kedro.io import CatalogProtocol
+    from kedro.pipeline import Pipeline
+    from kedro.pipeline.node import Node
+
+logger = logging.getLogger(__name__)
+
+
+def _resolve_output_path(catalog: CatalogProtocol, name: str) -> str:
+    """Best-effort extraction of a file path from a catalog dataset.
+
+    Returns the absolute path string for file-backed datasets, or ``""``
+    for in-memory or otherwise non-persistent datasets.
+    """
+    try:
+        dataset = getattr(catalog, "_datasets", {}).get(name)
+        if dataset is None:
+            return ""
+        for attr in ("_filepath", "filepath", "_path", "path"):
+            val = getattr(dataset, attr, None)
+            if val is not None:
+                return str(os.path.abspath(val))
+    except Exception:
+        logger.debug("Could not resolve file path for output '%s'.", name)
+    return ""
+
+
+@contextmanager
+def cache_transaction(registry: CacheRegistry, step_cache: StepCache):  # type: ignore[type-arg]
+    """Ensure cache entry is written only after a successful dataset save.
+
+    A ``.__pending__`` sentinel prevents stale reads if the process dies
+    between catalog.save() and registry.set().
+    """
+    sentinel = f"{step_cache.step_id}.__pending__"
+    try:
+        registry.set_pending(sentinel)
+        yield
+        registry.delete(sentinel)
+        registry.set(step_cache.step_id, step_cache)
+    except Exception:
+        registry.delete(sentinel)
+        raise
+
+
+class CachingHook:
+    """Pluggy hook implementation that provides pipeline-level node caching.
+
+    Lifecycle:
+    1. ``before_pipeline_run`` — initialises the dirty-node set.
+    2. ``is_cached(node, catalog, run_id)`` — called by the runner for each node
+       *before* execution to decide whether to skip.
+    3. ``after_node_run`` — registers a StepCache entry after successful execution.
+    4. ``on_node_error`` — cleans up any pending sentinel on failure.
+
+    Args:
+        registry: A ``CacheRegistry`` instance (filesystem or Redis).
+    """
+
+    def __init__(self, registry: CacheRegistry) -> None:
+        self._registry = registry
+        self._dirty_nodes: set[str] = set()
+        self._node_children: dict[str, set[str]] = {}
+        # Run-level context populated from run_params
+        self._runner_class: str = ""
+        self._config_env: str = "local"
+        self._cli_flags: dict[str, Any] = {}
+
+    @hook_impl
+    def before_pipeline_run(
+        self,
+        run_params: dict[str, Any],
+        pipeline: Pipeline,
+        catalog: CatalogProtocol,
+    ) -> None:
+        """Reset per-run state and build the node-children map."""
+        self._dirty_nodes = set()
+        self._node_children = {}
+
+        # Extract run-level metadata from KedroSession's record_data
+        self._runner_class = str(run_params.get("runner", ""))
+        self._config_env = str(run_params.get("env", "local"))
+        self._cli_flags = _mask_cli_flags(run_params.get("runtime_params", {}))
+
+        # Build output→node mapping, then children map
+        output_to_node: dict[str, str] = {}
+        for node in pipeline.nodes:
+            for output in node.outputs:
+                output_to_node[output] = node.name
+
+        for node in pipeline.nodes:
+            children: set[str] = set()
+            for inp in node.inputs:
+                if inp in output_to_node:
+                    children.add(output_to_node[inp])
+            # Reverse: parent → children
+            parent = output_to_node.get(
+                next((o for o in node.inputs if o in output_to_node), ""), ""
+            )
+            if parent:
+                self._node_children.setdefault(parent, set()).add(node.name)
+
+        # Actually build it correctly: for each node, find which nodes consume its outputs
+        self._node_children = {}
+        for node in pipeline.nodes:
+            self._node_children[node.name] = set()
+
+        for node in pipeline.nodes:
+            for inp in node.inputs:
+                if inp in output_to_node:
+                    parent_name = output_to_node[inp]
+                    self._node_children.setdefault(parent_name, set()).add(node.name)
+
+    def _mark_downstream_dirty(self, node_name: str) -> None:
+        """BFS: mark all nodes reachable from *node_name* as dirty."""
+        from collections import deque  # noqa: PLC0415
+
+        queue = deque(self._node_children.get(node_name, set()))
+        while queue:
+            child = queue.popleft()
+            if child not in self._dirty_nodes:
+                self._dirty_nodes.add(child)
+                queue.extend(self._node_children.get(child, set()))
+
+    def is_cached(
+        self,
+        node: Node,
+        catalog: CatalogProtocol,
+        run_id: str | None,
+    ) -> bool:
+        """Determine whether *node* can be skipped due to a valid cache entry.
+
+        Returns ``True`` (skip) or ``False`` (must execute).
+        """
+        if node.name in self._dirty_nodes:
+            logger.info(
+                "Node '%s' is dirty (upstream cache miss) — re-executing.", node.name
+            )
+            return False
+
+        stored = self._registry.get(node.name)
+        if stored is None:
+            logger.info("No cache entry for node '%s' — executing.", node.name)
+            self._mark_downstream_dirty(node.name)
+            return False
+
+        # Verify persisted outputs still exist on disk
+        for output_name, output_path in stored.output_paths.items():
+            if not output_path:
+                continue  # Non-file dataset; skip existence check
+            if not os.path.exists(output_path):
+                logger.info(
+                    "Cache miss for node '%s': output '%s' missing at %s — re-executing.",
+                    node.name,
+                    output_name,
+                    output_path,
+                )
+                self._mark_downstream_dirty(node.name)
+                return False
+
+        # Build current snapshot, passing stored so missing inputs
+        # (from skipped upstream nodes) fall back to stored hashes.
+        current = self._build_current_cache(node, catalog, run_id, stored=stored)
+        is_match, reason = stored.matches(current)
+        if not is_match:
+            logger.info(
+                "Cache miss for node '%s': %s — re-executing.", node.name, reason
+            )
+            self._mark_downstream_dirty(node.name)
+            return False
+
+        logger.info("Cache hit for node '%s' — skipping.", node.name)
+        return True
+
+    def _build_current_cache(
+        self,
+        node: Node,
+        catalog: CatalogProtocol,
+        run_id: str | None,
+        stored: StepCache | None = None,
+    ) -> StepCache:
+        """Construct a StepCache snapshot reflecting the *current* state.
+
+        If an input dataset is unavailable (e.g. produced by a skipped
+        upstream node whose output was released), we fall back to the
+        stored hash for that input so it compares as unchanged.
+        """
+        data_inputs: dict[str, Any] = {}
+        params: dict[str, Any] = {}
+        for inp_name in node.inputs:
+            try:
+                value = catalog.load(inp_name)
+            except Exception:
+                value = None
+            if inp_name.startswith("params:"):
+                params[inp_name] = value
+            else:
+                data_inputs[inp_name] = value
+
+        code_hash = CodeHasher.hash_function(node.func)
+        code_files = CodeHasher.file_hashes_for(node.func)
+
+        # If some inputs are unavailable (upstream was a cache hit and its
+        # output dataset was released), reuse the stored hashes so the
+        # comparison stays valid.
+        input_hash: str
+        if any(v is None for v in data_inputs.values()) and stored is not None:
+            input_hash = stored.input_hash
+        else:
+            input_hash = compute_input_hash(data_inputs)
+
+        parameter_hash: str
+        if any(v is None for v in params.values()) and stored is not None:
+            parameter_hash = stored.parameter_hash
+        else:
+            parameter_hash = compute_parameter_hash(params)
+
+        return StepCache(
+            step_id=node.name,
+            start_timestamp=datetime.now(tz=timezone.utc).isoformat(),
+            end_timestamp="",
+            session_id=run_id or "",
+            worker_id=os.environ.get("KEDRO_WORKER_ID", "main"),
+            runner_class=self._runner_class,
+            pipeline_namespace=node.namespace,
+            cli_flags=self._cli_flags,
+            config_env=self._config_env,
+            config_source="both",
+            code_hash=code_hash,
+            input_hash=input_hash,
+            parameter_hash=parameter_hash,
+            input_paths={k: "" for k in data_inputs},
+            input_schema={k: type(v).__name__ for k, v in data_inputs.items()},
+            code_files=code_files,
+        )
+
+    @hook_impl
+    def after_node_run(  # noqa: PLR0913
+        self,
+        node: Node,
+        catalog: CatalogProtocol,
+        inputs: dict[str, Any],
+        outputs: dict[str, Any],
+        is_async: bool,
+        run_id: str,
+    ) -> None:
+        """Register a StepCache entry after successful node execution."""
+        data_inputs = {k: v for k, v in inputs.items() if not k.startswith("params:")}
+        params = {k: v for k, v in inputs.items() if k.startswith("params:")}
+
+        code_hash = CodeHasher.hash_function(node.func)
+        code_files = CodeHasher.file_hashes_for(node.func)
+        now = datetime.now(tz=timezone.utc).isoformat()
+
+        # Check for an existing entry to increment cache_hits
+        existing = self._registry.get(node.name)
+        cache_hits = existing.cache_hits if existing else 0
+
+        # Resolve real file paths for persisted output datasets
+        resolved_output_paths = {k: _resolve_output_path(catalog, k) for k in outputs}
+
+        step_cache = StepCache(
+            step_id=node.name,
+            start_timestamp=now,
+            end_timestamp=now,
+            session_id=run_id or "",
+            worker_id=os.environ.get("KEDRO_WORKER_ID", "main"),
+            runner_class=self._runner_class,
+            pipeline_namespace=node.namespace,
+            cli_flags=self._cli_flags,
+            config_env=self._config_env,
+            config_source="both",
+            code_hash=code_hash,
+            input_hash=compute_input_hash(data_inputs),
+            parameter_hash=compute_parameter_hash(params),
+            input_paths={k: "" for k in data_inputs},
+            input_schema={k: type(v).__name__ for k, v in data_inputs.items()},
+            output_paths=resolved_output_paths,
+            cache_hits=cache_hits,
+            code_files=code_files,
+        )
+
+        with cache_transaction(self._registry, step_cache):
+            # Verify outputs exist (the save already happened in Task.execute)
+            for output_name in outputs:
+                logger.debug("Output '%s' saved successfully.", output_name)
+
+    @hook_impl
+    def on_node_error(  # noqa: PLR0913
+        self,
+        error: Exception,
+        node: Node,
+        catalog: CatalogProtocol,
+        inputs: dict[str, Any],
+        is_async: bool,
+        run_id: str,
+    ) -> None:
+        """Clean up any pending sentinel if the node failed."""
+        sentinel = f"{node.name}.__pending__"
+        self._registry.delete(sentinel)
+        logger.debug("Cleaned up pending sentinel for failed node '%s'.", node.name)
+
+    def increment_cache_hit(self, node_name: str) -> None:
+        """Atomically increment the cache_hits counter for a node.
+
+        Delegates to ``CacheRegistry.atomic_update`` so the read-modify-write
+        is protected by the backend's native concurrency mechanism (file lock
+        on ``FilesystemBackend``, CAS Lua script on ``RedisBackend``).
+        """
+
+        def _updater(existing: StepCache | None) -> StepCache | None:
+            if existing is None:
+                return None
+            return StepCache(
+                step_id=existing.step_id,
+                start_timestamp=existing.start_timestamp,
+                end_timestamp=existing.end_timestamp,
+                session_id=existing.session_id,
+                worker_id=existing.worker_id,
+                cache_hits=existing.cache_hits + 1,
+                runner_class=existing.runner_class,
+                pipeline_namespace=existing.pipeline_namespace,
+                cli_flags=existing.cli_flags,
+                config_env=existing.config_env,
+                config_source=existing.config_source,
+                code_hash=existing.code_hash,
+                input_hash=existing.input_hash,
+                parameter_hash=existing.parameter_hash,
+                input_paths=existing.input_paths,
+                input_schema=existing.input_schema,
+                output_paths=existing.output_paths,
+                save_version=existing.save_version,
+                code_files=existing.code_files,
+            )
+
+        self._registry.atomic_update(node_name, _updater)
diff --git a/kedro/cache/registry.py b/kedro/cache/registry.py
new file mode 100644
index 00000000..ba4b94f8
--- /dev/null
+++ b/kedro/cache/registry.py
@@ -0,0 +1,98 @@
+"""CacheRegistry ABC and factory for cache backends."""
+
+from __future__ import annotations
+
+from abc import ABC, abstractmethod
+from typing import TYPE_CHECKING
+
+if TYPE_CHECKING:
+    from collections.abc import Callable
+
+    from kedro.cache.step_cache import StepCache
+
+
+class CacheRegistry(ABC):
+    """Abstract base class for cache storage backends."""
+
+    @abstractmethod
+    def get(self, key: str) -> StepCache | None:
+        """Retrieve a cached StepCache by key, or ``None`` if absent."""
+
+    @abstractmethod
+    def set(self, key: str, step_cache: StepCache) -> None:
+        """Store a StepCache under the given key."""
+
+    @abstractmethod
+    def delete(self, key: str) -> None:
+        """Remove a cache entry. No-op if the key does not exist."""
+
+    def set_pending(self, sentinel: str) -> None:
+        """Mark a sentinel key indicating a save is in progress.
+
+        Default implementation stores an empty JSON placeholder via ``set``.
+        Backends may override for atomicity guarantees.
+        """
+        from kedro.cache.step_cache import StepCache  # noqa: PLC0415
+
+        placeholder = StepCache(
+            step_id=sentinel,
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+        )
+        self.set(sentinel, placeholder)
+
+    def atomic_update(
+        self,
+        key: str,
+        updater: Callable[[StepCache | None], StepCache | None],
+    ) -> None:
+        """Read, transform, and write a cache entry atomically.
+
+        *updater* receives the current ``StepCache`` (or ``None``) and returns
+        the desired new value (or ``None`` to delete the entry).
+
+        The default implementation is a naive read-then-write with no
+        concurrency protection.  Subclasses **must** override this with
+        backend-specific locking (file lock, Lua CAS, etc.) to guarantee
+        atomicity under concurrent access.
+        """
+        current = self.get(key)
+        updated = updater(current)
+        if updated is not None:
+            self.set(key, updated)
+        elif current is not None:
+            self.delete(key)
+
+
+def create_registry(
+    backend_type: str = "filesystem", **kwargs: object
+) -> CacheRegistry:
+    """Factory: instantiate a CacheRegistry by backend name.
+
+    Args:
+        backend_type: ``"filesystem"``, ``"redis"``, or ``"s3"``.
+        **kwargs: Forwarded to the chosen backend constructor.
+
+    Raises:
+        ValueError: If *backend_type* is not recognised.
+    """
+    if backend_type == "filesystem":
+        from kedro.cache.backends.filesystem_backend import (  # noqa: PLC0415
+            FilesystemBackend,
+        )
+
+        return FilesystemBackend(**kwargs)  # type: ignore[arg-type]
+    if backend_type == "redis":
+        from kedro.cache.backends.redis_backend import RedisBackend  # noqa: PLC0415
+
+        return RedisBackend(**kwargs)  # type: ignore[arg-type]
+    if backend_type == "s3":
+        from kedro.cache.backends.s3_backend import S3Backend  # noqa: PLC0415
+
+        return S3Backend(**kwargs)  # type: ignore[arg-type]
+    raise ValueError(
+        f"Unknown cache backend type: {backend_type!r}. "
+        "Supported: 'filesystem', 'redis', 's3'."
+    )
diff --git a/kedro/cache/step_cache.py b/kedro/cache/step_cache.py
new file mode 100644
index 00000000..90ccffa8
--- /dev/null
+++ b/kedro/cache/step_cache.py
@@ -0,0 +1,184 @@
+"""StepCache dataclass and validation logic for pipeline node caching."""
+
+from __future__ import annotations
+
+import hashlib
+import json
+import logging
+from dataclasses import asdict, dataclass, field
+from typing import Any
+
+logger = logging.getLogger(__name__)
+
+_CREDENTIAL_BLOCKLIST = frozenset(
+    {
+        "password",
+        "secret",
+        "token",
+        "credential",
+        "api_key",
+        "private_key",
+        "auth",
+        "passwd",
+        "pwd",
+    }
+)
+
+_INVALIDATION_FIELDS = frozenset(
+    {
+        "runner_class",
+        "pipeline_namespace",
+        "config_env",
+        "code_hash",
+        "input_hash",
+        "parameter_hash",
+        "input_paths",
+    }
+)
+
+
+def _mask_cli_flags(
+    flags: dict[str, Any],
+    blocklist: frozenset[str] = _CREDENTIAL_BLOCKLIST,
+) -> dict[str, Any]:
+    """Replace sensitive CLI flag values with a redacted sentinel."""
+    masked: dict[str, Any] = {}
+    for key, value in flags.items():
+        if any(blocked in key.lower() for blocked in blocklist):
+            masked[key] = "***REDACTED***"
+        else:
+            masked[key] = value
+    return masked
+
+
+@dataclass(frozen=True)
+class StepCache:
+    """Immutable snapshot of a pipeline node's execution context.
+
+    Fields split into three categories:
+    - *Step Info / Metadata* — informational only, do not trigger invalidation.
+    - *Environment* — changes here invalidate the cache.
+    - *Cache Invariants* — code, input, and parameter hashes.
+    - *Input / Output Metadata* — structural info for diagnostics.
+    """
+
+    # --- Step Info (metadata, no invalidation) ---
+    step_id: str
+    start_timestamp: str
+    end_timestamp: str
+    session_id: str
+    worker_id: str
+    cache_hits: int = 0
+
+    # --- Environment (invalidation triggers) ---
+    runner_class: str = ""
+    pipeline_namespace: str | None = None
+    cli_flags: dict[str, Any] = field(default_factory=dict)
+    config_env: str = "local"
+    config_source: str = "both"
+
+    # --- Cache Invariants (invalidation triggers) ---
+    code_hash: str = ""
+    input_hash: str = ""
+    parameter_hash: str = ""
+
+    # --- Input Metadata ---
+    input_paths: dict[str, str] = field(default_factory=dict)
+    input_schema: dict[str, str] = field(default_factory=dict)
+
+    # --- Output Metadata ---
+    output_paths: dict[str, str] = field(default_factory=dict)
+    save_version: str | None = None
+
+    # --- Per-file code hashes (for detailed miss diagnostics) ---
+    code_files: dict[str, str] = field(default_factory=dict)
+
+    def matches(self, other: StepCache) -> tuple[bool, str | None]:
+        """Compare this (stored) cache against *other* (current) on invalidation fields.
+
+        Returns:
+            A tuple of (is_match, reason). *reason* is ``None`` on a hit, or a
+            human-readable string describing the first field that differs.
+        """
+        for field_name in _INVALIDATION_FIELDS:
+            stored_val = getattr(self, field_name)
+            current_val = getattr(other, field_name)
+            if field_name == "input_paths":
+                # Only compare key sets for structural match
+                if set(stored_val.keys()) != set(current_val.keys()):
+                    return False, (
+                        f"input_paths key mismatch: "
+                        f"stored={sorted(stored_val.keys())}, "
+                        f"current={sorted(current_val.keys())}"
+                    )
+            elif field_name == "code_hash" and stored_val != current_val:
+                # Produce file-level detail when both sides have code_files
+                detail = _diff_code_files(self.code_files, other.code_files)
+                return False, f"code_hash changed ({detail})"
+            elif stored_val != current_val:
+                return False, (
+                    f"{field_name} changed: stored={stored_val!r}, "
+                    f"current={current_val!r}"
+                )
+        return True, None
+
+    def to_dict(self) -> dict[str, Any]:
+        """Serialize to a JSON-compatible dictionary."""
+        return asdict(self)
+
+    @classmethod
+    def from_dict(cls, data: dict[str, Any]) -> StepCache:
+        """Deserialize from a dictionary."""
+        return cls(**data)
+
+    def to_json(self) -> str:
+        """Serialize to a JSON string."""
+        return json.dumps(self.to_dict(), indent=2, default=str)
+
+    @classmethod
+    def from_json(cls, raw: str) -> StepCache:
+        """Deserialize from a JSON string."""
+        return cls.from_dict(json.loads(raw))
+
+
+def _diff_code_files(stored: dict[str, str], current: dict[str, str]) -> str:
+    """Produce a human-readable summary of which files changed between two code_files maps."""
+    if not stored and not current:
+        return "hash differs (no per-file detail available)"
+    if not stored or not current:
+        return "hash differs (per-file detail unavailable on one side)"
+
+    changed = sorted(f for f in current if stored.get(f) != current[f] and f in stored)
+    added = sorted(f for f in current if f not in stored)
+    removed = sorted(f for f in stored if f not in current)
+
+    parts: list[str] = []
+    if changed:
+        parts.append(f"changed in {', '.join(changed)}")
+    if added:
+        parts.append(f"new file {', '.join(added)}")
+    if removed:
+        parts.append(f"removed {', '.join(removed)}")
+    return (
+        "; ".join(parts)
+        if parts
+        else "hash differs (all files match — possible AST-level change)"
+    )
+
+
+def compute_input_hash(inputs: dict[str, Any]) -> str:
+    """Compute a SHA-256 hash over the sorted, serialized input values."""
+    h = hashlib.sha256()
+    for key in sorted(inputs.keys()):
+        value_repr = repr(inputs[key])
+        h.update(f"{key}:{value_repr}".encode())
+    return h.hexdigest()
+
+
+def compute_parameter_hash(parameters: dict[str, Any]) -> str:
+    """Compute a SHA-256 hash over the sorted, serialized parameters."""
+    h = hashlib.sha256()
+    for key in sorted(parameters.keys()):
+        value_repr = repr(parameters[key])
+        h.update(f"{key}:{value_repr}".encode())
+    return h.hexdigest()
diff --git a/kedro/runner/runner.py b/kedro/runner/runner.py
index ee126bdf..7d7afd80 100644
--- a/kedro/runner/runner.py
+++ b/kedro/runner/runner.py
@@ -32,6 +32,7 @@ if TYPE_CHECKING:
 
     from pluggy import PluginManager
 
+    from kedro.cache.hook import CachingHook
     from kedro.io import CatalogProtocol, SharedMemoryCatalogProtocol
     from kedro.pipeline.node import Node
 
@@ -195,7 +196,7 @@ class AbstractRunner(ABC):
         pass
 
     @abstractmethod  # pragma: no cover
-    def _run(
+    def _run(  # noqa: PLR0915
         self,
         pipeline: Pipeline,
         catalog: CatalogProtocol | SharedMemoryCatalogProtocol,
@@ -227,10 +228,24 @@ class AbstractRunner(ABC):
         done = None
         max_workers = self._get_required_workers_count(pipeline)
 
+        dirty_nodes: set[Node] = set()
+
         pool = self._get_executor(max_workers)
         if pool is None:
             for exec_index, node in enumerate(nodes):
                 try:
+                    if _is_node_cached(
+                        node, catalog, hook_manager, run_id, dirty_nodes
+                    ):
+                        done_nodes.add(node)
+                        self._logger.info("Skipped node (cache hit): %s", node.name)
+                        self._logger.info(
+                            "Completed %d out of %d tasks",
+                            len(done_nodes),
+                            len(nodes),
+                        )
+                        self._release_datasets(node, catalog, load_counts, pipeline)
+                        continue
                     Task(
                         node=node,
                         catalog=catalog,
@@ -255,6 +270,13 @@ class AbstractRunner(ABC):
                 ready = {n for n in todo_nodes if node_dependencies[n] <= done_nodes}
                 todo_nodes -= ready
                 for node in ready:
+                    if _is_node_cached(
+                        node, catalog, hook_manager, run_id, dirty_nodes
+                    ):
+                        done_nodes.add(node)
+                        self._logger.info("Skipped node (cache hit): %s", node.name)
+                        self._release_datasets(node, catalog, load_counts, pipeline)
+                        continue
                     task = Task(
                         node=node,
                         catalog=catalog,
@@ -715,3 +737,52 @@ def _is_dataset_ephemeral_or_missing(
 
     # Persistent datasets need to be made if they don't exist
     return not catalog.exists(dataset_name)
+
+
+def _extract_caching_hook(
+    hook_manager: PluginManager | None,
+) -> CachingHook | None:
+    """Extract a CachingHook instance from the plugin manager, if registered."""
+    if hook_manager is None:
+        return None
+    plugins = hook_manager.get_plugins()
+    if not plugins:
+        return None
+    for plugin in plugins:
+        if isinstance(plugin, _CachingHookType()):
+            return plugin  # type: ignore[return-value]
+    return None
+
+
+def _CachingHookType() -> type:
+    """Lazy import guard for CachingHook type."""
+    from kedro.cache.hook import CachingHook  # noqa: PLC0415
+
+    return CachingHook
+
+
+def _is_node_cached(
+    node: Node,
+    catalog: CatalogProtocol | SharedMemoryCatalogProtocol,
+    hook_manager: PluginManager | None,
+    run_id: str | None,
+    dirty_nodes: set[Node],
+) -> bool:
+    """Check whether *node* should be skipped due to a valid cache entry.
+
+    Returns ``False`` immediately (no-op) if no ``CachingHook`` is registered.
+    When a cache miss is detected, all downstream nodes are added to *dirty_nodes*.
+    """
+    caching_hook = _extract_caching_hook(hook_manager)
+    if caching_hook is None:
+        return False
+
+    # Check if this node was marked dirty by an upstream miss
+    if node.name in caching_hook._dirty_nodes:
+        dirty_nodes.add(node)
+        return False
+
+    is_cached = caching_hook.is_cached(node, catalog, run_id)
+    if is_cached:
+        caching_hook.increment_cache_hit(node.name)
+    return is_cached
diff --git a/pyproject.toml b/pyproject.toml
index 53fd0ca1..e1fdf303 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -68,6 +68,7 @@ test = [
     "pytest-mock>=1.7.1,<4.0",
     "pytest-xdist[psutil]>=2.2.1,<4.0",
     "pytest>=7.2,<10.0",
+    "redis>=4.0.0",
     "requests_mock",
     "s3fs>=2021.4,<2026.2",
     # mypy related dependencies
@@ -176,6 +177,7 @@ layers = [
     "framework.context",
     "framework.project",
     "runner",
+    "cache",
     "io",
     "pipeline",
     "config"
@@ -183,7 +185,8 @@ layers = [
 ignore_imports = [
     "kedro.runner.task -> kedro.framework.project",
     "kedro.framework.hooks.specs -> kedro.framework.context",
-    "kedro -> kedro.ipython"
+    "kedro -> kedro.ipython",
+    "kedro.cache.hook -> kedro.framework.hooks"
 ]
 
 [[tool.importlinter.contracts]]
@@ -207,6 +210,7 @@ forbidden_modules = [
     "kedro.runner",
     "kedro.io",
     "kedro.pipeline",
+    "kedro.cache",
 ]
 
 [[tool.importlinter.contracts]]
@@ -216,6 +220,7 @@ source_modules = [
     "kedro.runner",
     "kedro.io",
     "kedro.pipeline",
+    "kedro.cache",
 ]
 forbidden_modules = [
     "kedro.config"
diff --git a/tests/cache/__init__.py b/tests/cache/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/tests/cache/conftest.py b/tests/cache/conftest.py
new file mode 100644
index 00000000..c455930e
--- /dev/null
+++ b/tests/cache/conftest.py
@@ -0,0 +1,90 @@
+"""Shared fixtures for cache tests."""
+
+from __future__ import annotations
+
+import time
+
+import pytest
+
+from kedro.cache.backends.filesystem_backend import FilesystemBackend
+from kedro.cache.hook import CachingHook
+from kedro.io import DataCatalog, MemoryDataset
+from kedro.pipeline import Pipeline, node
+
+# ---------------------------------------------------------------------------
+# Step functions (defined at module level so inspect can resolve source)
+# ---------------------------------------------------------------------------
+
+
+def add_numbers(a, b):
+    """Step A: simple addition."""
+    return {"sum": a + b}
+
+
+def square_and_wait(sum, wait_seconds):
+    """Step B: square with a deliberate delay for timing assertions."""
+    time.sleep(wait_seconds)
+    return {"squared": sum * sum}
+
+
+def write_report(squared):
+    """Step C: produce a text report."""
+    return {"report": f"Result: {squared}"}
+
+
+# ---------------------------------------------------------------------------
+# Fixtures
+# ---------------------------------------------------------------------------
+
+
+@pytest.fixture
+def registry_fixture(tmp_path):
+    """Filesystem-backed registry using a temp file."""
+    cache_file = str(tmp_path / ".kedro_cache.json")
+    return FilesystemBackend(path=cache_file)
+
+
+@pytest.fixture(params=["Worker-1", "Worker-2"])
+def worker_id_fixture(request, monkeypatch):
+    """Parametrized worker-ID environment variable."""
+    monkeypatch.setenv("KEDRO_WORKER_ID", request.param)
+    return request.param
+
+
+@pytest.fixture
+def pipeline_fixture():
+    """3-node linear pipeline: STEP_A -> STEP_B -> STEP_C."""
+    return Pipeline(
+        [
+            node(add_numbers, inputs=["a", "b"], outputs="sum", name="add_numbers"),
+            node(
+                square_and_wait,
+                inputs=["sum", "wait_seconds"],
+                outputs="squared",
+                name="square_and_wait",
+            ),
+            node(write_report, inputs="squared", outputs="report", name="write_report"),
+        ]
+    )
+
+
+@pytest.fixture
+def catalog_fixture():
+    """In-memory catalog with all required datasets."""
+    catalog = DataCatalog(
+        datasets={
+            "a": MemoryDataset(data=3),
+            "b": MemoryDataset(data=4),
+            "wait_seconds": MemoryDataset(data=0),
+            "sum": MemoryDataset(),
+            "squared": MemoryDataset(),
+            "report": MemoryDataset(),
+        }
+    )
+    return catalog
+
+
+@pytest.fixture
+def caching_hook_fixture(registry_fixture):
+    """Pre-built CachingHook wired to the filesystem registry."""
+    return CachingHook(registry=registry_fixture)
diff --git a/tests/cache/shared_utility.py b/tests/cache/shared_utility.py
new file mode 100644
index 00000000..96290496
--- /dev/null
+++ b/tests/cache/shared_utility.py
@@ -0,0 +1,14 @@
+"""Shared utility imported by test step functions to exercise transitive hashing."""
+
+
+def helper_add(a, b):
+    """Simple addition helper."""
+    return a + b
+
+
+def helper_square(x):
+    """Simple squaring helper."""
+    return x * x
+
+
+CONSTANT = 42
diff --git a/tests/cache/test_cache_integration.py b/tests/cache/test_cache_integration.py
new file mode 100644
index 00000000..9488ce10
--- /dev/null
+++ b/tests/cache/test_cache_integration.py
@@ -0,0 +1,378 @@
+"""End-to-end integration tests for the pipeline caching system.
+
+Covers §6.2 (cache hit), §6.3 (cache miss per invalidation field),
+and §6.4 (lazy hashing performance).
+"""
+
+from __future__ import annotations
+
+import importlib.util
+import sys
+import time
+from time import perf_counter
+
+import pluggy
+
+from kedro.cache.code_hasher import CodeHasher
+from kedro.cache.hook import CachingHook
+from kedro.framework.hooks.specs import (
+    DataCatalogSpecs,
+    DatasetSpecs,
+    KedroContextSpecs,
+    NodeSpecs,
+    PipelineSpecs,
+)
+from kedro.io import DataCatalog, MemoryDataset
+from kedro.pipeline import Pipeline, node
+from kedro.runner import SequentialRunner
+
+# ---------------------------------------------------------------------------
+# Step functions
+# ---------------------------------------------------------------------------
+
+
+def add_numbers(a, b):
+    return a + b
+
+
+def square_and_wait(sum, wait_seconds):
+    time.sleep(wait_seconds)
+    return sum * sum
+
+
+def write_report(squared):
+    return f"Result: {squared}"
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+
+def _make_pipeline():
+    return Pipeline(
+        [
+            node(add_numbers, inputs=["a", "b"], outputs="sum", name="add_numbers"),
+            node(
+                square_and_wait,
+                inputs=["sum", "wait_seconds"],
+                outputs="squared",
+                name="square_and_wait",
+            ),
+            node(write_report, inputs="squared", outputs="report", name="write_report"),
+        ]
+    )
+
+
+def _make_catalog(wait_seconds=0):
+    return DataCatalog(
+        datasets={
+            "a": MemoryDataset(data=3),
+            "b": MemoryDataset(data=4),
+            "wait_seconds": MemoryDataset(data=wait_seconds),
+            "sum": MemoryDataset(),
+            "squared": MemoryDataset(),
+            "report": MemoryDataset(),
+        }
+    )
+
+
+def _run_with_hook(pipeline, catalog, hook):
+    """Run pipeline with a CachingHook registered via pluggy."""
+    pm = pluggy.PluginManager("kedro")
+    pm.add_hookspecs(PipelineSpecs)
+    pm.add_hookspecs(NodeSpecs)
+    pm.add_hookspecs(DataCatalogSpecs)
+    pm.add_hookspecs(DatasetSpecs)
+    pm.add_hookspecs(KedroContextSpecs)
+    pm.register(hook)
+
+    runner = SequentialRunner()
+    return runner.run(pipeline, catalog, hook_manager=pm)
+
+
+# ---------------------------------------------------------------------------
+# §6.2 Cache Hit
+# ---------------------------------------------------------------------------
+
+
+class TestCacheHit:
+    def test_cache_hit_skips_execution_and_is_fast(self, registry_fixture):
+        CodeHasher.reset()
+        hook = CachingHook(registry=registry_fixture)
+        pipeline = _make_pipeline()
+
+        # Run 1: cold — all nodes execute
+        catalog = _make_catalog(wait_seconds=0)
+        _run_with_hook(pipeline, catalog, hook)
+
+        # Verify all 3 nodes registered
+        assert registry_fixture.get("add_numbers") is not None
+        assert registry_fixture.get("square_and_wait") is not None
+        assert registry_fixture.get("write_report") is not None
+
+        # Run 2: warm — reuse same catalog so MemoryDatasets still hold data
+        t0 = perf_counter()
+        _run_with_hook(pipeline, catalog, hook)
+        duration = perf_counter() - t0
+        assert duration < 2.0  # cache overhead only
+
+        # cache_hits incremented for all nodes
+        assert registry_fixture.get("add_numbers").cache_hits >= 1
+        assert registry_fixture.get("square_and_wait").cache_hits >= 1
+        assert registry_fixture.get("write_report").cache_hits >= 1
+
+    def test_cache_hit_with_delay_shows_speedup(self, registry_fixture):
+        CodeHasher.reset()
+        hook = CachingHook(registry=registry_fixture)
+        pipeline = _make_pipeline()
+
+        # Run 1: cold with 0.5s delay
+        catalog = _make_catalog(wait_seconds=0.5)
+        t0 = perf_counter()
+        _run_with_hook(pipeline, catalog, hook)
+        first_run = perf_counter() - t0
+        assert first_run >= 0.5
+
+        # Run 2: warm — reuse same catalog so MemoryDatasets still hold data
+        t0 = perf_counter()
+        _run_with_hook(pipeline, catalog, hook)
+        second_run = perf_counter() - t0
+        assert second_run < first_run
+
+
+# ---------------------------------------------------------------------------
+# §6.3 Cache Miss on Invalidation Field Changes
+# ---------------------------------------------------------------------------
+
+
+class TestCacheMissInputChange:
+    def test_cache_miss_on_input_change(self, registry_fixture):
+        CodeHasher.reset()
+        hook = CachingHook(registry=registry_fixture)
+        pipeline = _make_pipeline()
+
+        # Run 1: populate cache
+        catalog = _make_catalog(wait_seconds=0)
+        _run_with_hook(pipeline, catalog, hook)
+        hash1 = registry_fixture.get("add_numbers").input_hash
+
+        # Run 2: change input 'a' — use a fresh catalog with new value
+        # All inputs must be present so the runner can execute the miss nodes
+        catalog2 = DataCatalog(
+            datasets={
+                "a": MemoryDataset(data=99),  # changed
+                "b": MemoryDataset(data=4),
+                "wait_seconds": MemoryDataset(data=0),
+                "sum": MemoryDataset(),
+                "squared": MemoryDataset(),
+                "report": MemoryDataset(),
+            }
+        )
+        _run_with_hook(pipeline, catalog2, hook)
+
+        # add_numbers re-executed with a different input_hash
+        hash2 = registry_fixture.get("add_numbers").input_hash
+        assert hash1 != hash2
+
+
+class TestCacheMissParameterChange:
+    def test_cache_miss_on_parameter_change(self, registry_fixture):
+        CodeHasher.reset()
+        hook = CachingHook(registry=registry_fixture)
+
+        # Use a pipeline where a params: prefixed input triggers parameter_hash
+        def param_step(lr):
+            return lr * 2
+
+        pipeline = Pipeline(
+            [
+                node(
+                    param_step,
+                    inputs="params:lr",
+                    outputs="model",
+                    name="param_step",
+                )
+            ]
+        )
+        catalog = DataCatalog(
+            datasets={
+                "params:lr": MemoryDataset(data=0.01),
+                "model": MemoryDataset(),
+            }
+        )
+
+        # Run 1
+        _run_with_hook(pipeline, catalog, hook)
+        entry1 = registry_fixture.get("param_step")
+        assert entry1 is not None
+        param_hash1 = entry1.parameter_hash
+
+        # Run 2: change parameter value
+        catalog2 = DataCatalog(
+            datasets={
+                "params:lr": MemoryDataset(data=0.001),  # changed
+                "model": MemoryDataset(),
+            }
+        )
+        _run_with_hook(pipeline, catalog2, hook)
+
+        # parameter_hash should differ — node re-executed
+        entry2 = registry_fixture.get("param_step")
+        assert entry2 is not None
+        assert entry2.parameter_hash != param_hash1
+
+
+class TestCacheMissCodeChange:
+    def test_cache_miss_on_code_change(self, tmp_path, registry_fixture):
+        """Modifying a node's source code invalidates its cache."""
+        CodeHasher.reset()
+
+        # Create a dynamic step function in a temp file
+        step_file = tmp_path / "dynamic_step.py"
+        step_file.write_text("def dynamic_fn(x):\n    return x + 1\n")
+        (tmp_path / "pyproject.toml").write_text("[project]\nname='test'\n")
+
+        def _load_module():
+            spec = importlib.util.spec_from_file_location(
+                "dynamic_step", str(step_file)
+            )
+            mod = importlib.util.module_from_spec(spec)
+            sys.modules["dynamic_step"] = mod
+            spec.loader.exec_module(mod)
+            return mod
+
+        mod = _load_module()
+
+        pipeline = Pipeline(
+            [node(mod.dynamic_fn, inputs="x", outputs="out", name="dynamic_node")]
+        )
+        catalog = DataCatalog(
+            datasets={
+                "x": MemoryDataset(data=5),
+                "out": MemoryDataset(),
+            }
+        )
+
+        hook = CachingHook(registry=registry_fixture)
+        _run_with_hook(pipeline, catalog, hook)
+        hash1 = registry_fixture.get("dynamic_node").code_hash
+
+        # Modify the source
+        time.sleep(0.05)
+        step_file.write_text("def dynamic_fn(x):\n    return x + 100\n")
+        CodeHasher.reset()
+
+        # Reload module
+        mod2 = _load_module()
+        pipeline2 = Pipeline(
+            [node(mod2.dynamic_fn, inputs="x", outputs="out", name="dynamic_node")]
+        )
+        catalog2 = DataCatalog(
+            datasets={
+                "x": MemoryDataset(data=5),
+                "out": MemoryDataset(),
+            }
+        )
+
+        hook2 = CachingHook(registry=registry_fixture)
+        _run_with_hook(pipeline2, catalog2, hook2)
+        hash2 = registry_fixture.get("dynamic_node").code_hash
+
+        assert hash1 != hash2
+
+
+# ---------------------------------------------------------------------------
+# §6.4 Lazy Hashing Performance
+# ---------------------------------------------------------------------------
+
+
+class TestLazyHashing:
+    def test_shared_module_parsed_once(self, tmp_path):
+        """A module imported by all step functions is parsed exactly once."""
+        CodeHasher.reset()
+
+        shared = tmp_path / "shared_util.py"
+        shared.write_text("CONST = 42\n\ndef util(x):\n    return x + CONST\n")
+        (tmp_path / "pyproject.toml").write_text("[project]\nname='test'\n")
+
+        # tmp_path must be on sys.path so `import shared_util` resolves
+        sys.path.insert(0, str(tmp_path))
+        try:
+            funcs = []
+            for i in range(3):
+                mod_file = tmp_path / f"step_{i}.py"
+                mod_file.write_text(
+                    f"import shared_util\n\ndef step_{i}(x):\n"
+                    f"    return shared_util.util(x)\n"
+                )
+                spec = importlib.util.spec_from_file_location(
+                    f"step_{i}", str(mod_file)
+                )
+                mod = importlib.util.module_from_spec(spec)
+                sys.modules[f"step_{i}"] = mod
+                spec.loader.exec_module(mod)
+                funcs.append(getattr(mod, f"step_{i}"))
+
+            # Hash all 3 functions
+            for fn in funcs:
+                CodeHasher.hash_function(fn)
+
+            # shared_util.py should have been parsed exactly once
+            assert CodeHasher.hash_call_count_for("shared_util.py") == 1
+        finally:
+            sys.path.remove(str(tmp_path))
+
+    def test_large_pipeline_cache_check_performance(self, registry_fixture):
+        """100-node pipeline cache-check overhead should be under 5 seconds."""
+        CodeHasher.reset()
+
+        # Build 100 simple nodes — each returns a plain value (single output).
+        # exec() creates functions with the correct parameter signature so
+        # Kedro's input validation passes.
+        nodes = []
+        for i in range(100):
+
+            def make_fn(idx):
+                ns: dict = {}
+                if idx == 0:
+                    exec(  # noqa: S102
+                        f"def step_{idx}():\n    return {idx}\n", ns
+                    )
+                else:
+                    exec(  # noqa: S102
+                        f"def step_{idx}(out_{idx - 1}):\n    return {idx}\n",
+                        ns,
+                    )
+                return ns[f"step_{idx}"]
+
+            fn = make_fn(i)
+            if i == 0:
+                nodes.append(node(fn, inputs=[], outputs=f"out_{i}", name=f"step_{i}"))
+            else:
+                nodes.append(
+                    node(
+                        fn,
+                        inputs=[f"out_{i - 1}"],
+                        outputs=f"out_{i}",
+                        name=f"step_{i}",
+                    )
+                )
+
+        pipeline = Pipeline(nodes)
+
+        # Build catalog
+        datasets = {f"out_{i}": MemoryDataset() for i in range(100)}
+        catalog = DataCatalog(datasets=datasets)
+
+        # First run: populate cache
+        hook = CachingHook(registry=registry_fixture)
+        _run_with_hook(pipeline, catalog, hook)
+
+        # Second run: reuse same catalog so outputs are still available
+        hook2 = CachingHook(registry=registry_fixture)
+        t0 = perf_counter()
+        _run_with_hook(pipeline, catalog, hook2)
+        duration = perf_counter() - t0
+
+        assert duration < 5.0  # generous bound for CI environments
diff --git a/tests/cache/test_code_hasher.py b/tests/cache/test_code_hasher.py
new file mode 100644
index 00000000..773192ce
--- /dev/null
+++ b/tests/cache/test_code_hasher.py
@@ -0,0 +1,383 @@
+"""Tests for CodeHasher: AST hashing, mtime caching, transitive imports,
+SharedHashCache cross-process cache, and deadlock-freedom under recursion."""
+
+from __future__ import annotations
+
+import os
+import threading
+import time
+
+from kedro.cache.code_hasher import CodeHasher, SharedHashCache
+
+# ---------------------------------------------------------------------------
+# Simple test functions at module level (so inspect can resolve source)
+# ---------------------------------------------------------------------------
+
+
+def simple_func(x):
+    return x + 1
+
+
+def simple_func_v2(x):
+    return x + 2
+
+
+def func_with_comment(x):
+    # This comment should not affect the hash
+    return x + 1
+
+
+class TestCodeHasherBasics:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_hash_is_deterministic(self):
+        h1 = CodeHasher.hash_function(simple_func)
+        h2 = CodeHasher.hash_function(simple_func)
+        assert h1 == h2
+
+    def test_different_functions_different_hash(self):
+        h1 = CodeHasher.hash_function(simple_func)
+        h2 = CodeHasher.hash_function(simple_func_v2)
+        assert h1 != h2
+
+    def test_hash_returns_hex_string(self):
+        h = CodeHasher.hash_function(simple_func)
+        assert isinstance(h, str)
+        assert len(h) == 64  # SHA-256 hex digest
+        int(h, 16)  # must be valid hex
+
+
+class TestCodeHasherCaching:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_mtime_cache_prevents_reparse(self):
+        # First call parses the file
+        CodeHasher.hash_function(simple_func)
+        count_after_first = CodeHasher._hash_call_count
+
+        # Second call should hit cache (same mtime)
+        CodeHasher.hash_function(simple_func)
+        assert CodeHasher._hash_call_count == count_after_first
+
+    def test_reset_clears_cache(self):
+        CodeHasher.hash_function(simple_func)
+        assert CodeHasher._hash_call_count > 0
+        CodeHasher.reset()
+        assert CodeHasher._hash_call_count == 0
+        assert len(CodeHasher._file_cache) == 0
+
+    def test_hash_call_count_increments_on_miss(self):
+        CodeHasher.reset()
+        initial = CodeHasher._hash_call_count
+        CodeHasher.hash_function(simple_func)
+        assert CodeHasher._hash_call_count > initial
+
+
+class TestCodeHasherTransitiveImports:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_transitive_import_included_in_hash(self, tmp_path):
+        """Modifying a transitively imported module changes the hash."""
+        # Create a helper module
+        helper = tmp_path / "helper_mod.py"
+        helper.write_text("def assist(x):\n    return x\n")
+
+        # Create a main module that imports the helper
+        main = tmp_path / "main_mod.py"
+        main.write_text(
+            "import helper_mod\n\ndef process(x):\n    return helper_mod.assist(x)\n"
+        )
+
+        # Ensure tmp_path has a pyproject.toml so project_root detection works
+        (tmp_path / "pyproject.toml").write_text("[project]\nname = 'test'\n")
+
+        # Hash the main module
+        h1 = CodeHasher._hash_file(str(main), str(tmp_path))
+
+        # Modify helper (touch to change mtime)
+        time.sleep(0.05)
+        helper.write_text("def assist(x):\n    return x + 100\n")
+        CodeHasher.reset()
+
+        # Re-hash — should differ due to transitive change
+        h2 = CodeHasher._hash_file(str(main), str(tmp_path))
+        assert h1 != h2
+
+
+class TestCodeHasherBuiltins:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_builtin_function_does_not_crash(self):
+        # Built-in functions like len cannot have source inspected
+        h = CodeHasher.hash_function(len)
+        assert isinstance(h, str)
+        assert len(h) == 64
+
+    def test_lambda_function(self):
+        fn = lambda x: x + 1  # noqa: E731
+        h = CodeHasher.hash_function(fn)
+        assert isinstance(h, str)
+
+
+class TestCodeHasherFileCounts:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_hash_call_count_for_specific_file(self):
+        CodeHasher.hash_function(simple_func)
+        filename = os.path.basename(__file__)
+        assert CodeHasher.hash_call_count_for(filename) == 1
+
+    def test_repeated_calls_do_not_increment(self):
+        CodeHasher.hash_function(simple_func)
+        CodeHasher.hash_function(simple_func_v2)  # same file
+        filename = os.path.basename(__file__)
+        # File parsed once, cached for second call
+        assert CodeHasher.hash_call_count_for(filename) == 1
+
+
+class TestCodeHasherSharedModule:
+    """Verify that a shared module is parsed exactly once regardless of how
+    many functions import it."""
+
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_shared_module_parsed_once(self, tmp_path):
+        # Create shared module
+        shared = tmp_path / "shared_util.py"
+        shared.write_text("CONST = 1\n\ndef util():\n    return CONST\n")
+
+        (tmp_path / "pyproject.toml").write_text("[project]\nname = 'test'\n")
+
+        # Create 3 modules all importing the shared one
+        for i in range(3):
+            mod = tmp_path / f"step_{i}.py"
+            mod.write_text(
+                f"import shared_util\n\ndef step_{i}(x):\n"
+                f"    return shared_util.util() + x\n"
+            )
+
+        # Hash all 3
+        for i in range(3):
+            mod_path = str(tmp_path / f"step_{i}.py")
+            CodeHasher._hash_file(mod_path, str(tmp_path))
+
+        # shared_util.py should have been parsed exactly once
+        assert CodeHasher.hash_call_count_for("shared_util.py") == 1
+
+
+# ---------------------------------------------------------------------------
+# SharedHashCache unit tests
+# ---------------------------------------------------------------------------
+
+
+class TestSharedHashCache:
+    def test_get_returns_none_on_empty(self, tmp_path):
+        cache = SharedHashCache(str(tmp_path / "hashes.json"))
+        assert cache.get("missing") is None
+
+    def test_set_and_get_roundtrip(self, tmp_path):
+        cache = SharedHashCache(str(tmp_path / "hashes.json"))
+        cache.set("key1", "deadbeef")
+        assert cache.get("key1") == "deadbeef"
+
+    def test_get_or_compute_calls_fn_on_miss(self, tmp_path):
+        cache = SharedHashCache(str(tmp_path / "hashes.json"))
+        calls = []
+
+        def _compute():
+            calls.append(1)
+            return "computed_hash"
+
+        result = cache.get_or_compute("k", _compute)
+        assert result == "computed_hash"
+        assert len(calls) == 1
+
+    def test_get_or_compute_returns_cached_without_calling_fn(self, tmp_path):
+        cache = SharedHashCache(str(tmp_path / "hashes.json"))
+        cache.set("k", "precomputed")
+        calls = []
+
+        result = cache.get_or_compute("k", lambda: (calls.append(1) or "x"))
+        assert result == "precomputed"
+        assert calls == []
+
+    def test_clear_removes_file(self, tmp_path):
+        path = tmp_path / "hashes.json"
+        cache = SharedHashCache(str(path))
+        cache.set("k", "v")
+        assert path.exists()
+        cache.clear()
+        assert not path.exists()
+
+
+class TestSharedHashCacheConcurrency:
+    """Concurrent threads must not lose entries or corrupt the JSON file."""
+
+    def test_concurrent_set_all_persisted(self, tmp_path):
+        cache = SharedHashCache(str(tmp_path / "concurrent_hashes.json"))
+        n_threads = 30
+        errors: list[Exception] = []
+
+        def _writer(idx):
+            try:
+                cache.set(f"key_{idx}", f"hash_{idx}")
+            except Exception as exc:
+                errors.append(exc)
+
+        threads = [
+            threading.Thread(target=_writer, args=(i,)) for i in range(n_threads)
+        ]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        assert errors == []
+        for i in range(n_threads):
+            assert cache.get(f"key_{i}") == f"hash_{i}"
+
+    def test_get_or_compute_no_deadlock_under_recursion(self, tmp_path):
+        """Simulate the recursive _hash_file pattern: compute_fn itself
+        calls get_or_compute for a different key.  This must not deadlock
+        because the lock is released before compute_fn runs."""
+        cache = SharedHashCache(str(tmp_path / "recursive_hashes.json"))
+
+        def _outer_compute():
+            # Simulate following an import — acquires/releases lock independently
+            return cache.get_or_compute("inner_key", lambda: "inner_val")
+
+        result = cache.get_or_compute("outer_key", _outer_compute)
+        # outer result equals the inner value it computed
+        assert result == "inner_val"
+        assert cache.get("inner_key") == "inner_val"
+        assert cache.get("outer_key") == "inner_val"
+
+
+# ---------------------------------------------------------------------------
+# CodeHasher with shared cache integration
+# ---------------------------------------------------------------------------
+
+
+class TestCodeHasherWithSharedCache:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_configure_and_use_shared_cache(self, tmp_path):
+        cache_path = str(tmp_path / "shared_code_hashes.json")
+        CodeHasher.configure_shared_cache(cache_path)
+
+        h1 = CodeHasher.hash_function(simple_func)
+        # Should be in the shared file now
+        cache = SharedHashCache(cache_path)
+        # At least one entry should exist
+        store = cache._load()
+        assert len(store) > 0
+
+        # Second call hits L1 (in-process), no additional parse
+        count_before = CodeHasher._hash_call_count
+        h2 = CodeHasher.hash_function(simple_func)
+        assert h1 == h2
+        assert CodeHasher._hash_call_count == count_before
+
+    def test_shared_cache_survives_l1_clear(self, tmp_path):
+        """After clearing L1, the shared file still serves hits."""
+        cache_path = str(tmp_path / "shared_code_hashes.json")
+        CodeHasher.configure_shared_cache(cache_path)
+
+        h1 = CodeHasher.hash_function(simple_func)
+        count_after_first = CodeHasher._hash_call_count
+
+        # Clear only L1
+        CodeHasher._file_cache.clear()
+
+        # Re-hash — should hit L2 (shared file), not re-parse
+        h2 = CodeHasher.hash_function(simple_func)
+        assert h1 == h2
+        assert CodeHasher._hash_call_count == count_after_first
+
+    def test_reset_clears_shared_cache_file(self, tmp_path):
+        cache_path = tmp_path / "shared_code_hashes.json"
+        CodeHasher.configure_shared_cache(str(cache_path))
+        CodeHasher.hash_function(simple_func)
+        assert cache_path.exists()
+
+        CodeHasher.reset()
+        assert not cache_path.exists()
+        assert CodeHasher._shared_cache is None
+
+    def test_shared_cache_transitive_imports_no_deadlock(self, tmp_path):
+        """Hash a file that imports another — both go through SharedHashCache
+        without deadlocking (lock released before recursive compute)."""
+        (tmp_path / "pyproject.toml").write_text("[project]\nname='test'\n")
+        helper = tmp_path / "dep_mod.py"
+        helper.write_text("VAL = 1\n")
+        main = tmp_path / "main_mod.py"
+        main.write_text("import dep_mod\n\ndef fn():\n    return dep_mod.VAL\n")
+
+        cache_path = str(tmp_path / "hash_cache.json")
+        CodeHasher.configure_shared_cache(cache_path)
+
+        # Must not deadlock
+        digest = CodeHasher._hash_file(str(main), str(tmp_path))
+        assert isinstance(digest, str) and len(digest) == 64
+
+
+# ---------------------------------------------------------------------------
+# file_hashes_for — per-file breakdown for miss diagnostics
+# ---------------------------------------------------------------------------
+
+
+class TestFileHashesFor:
+    def setup_method(self):
+        CodeHasher.reset()
+
+    def test_returns_dict_with_source_file(self):
+        result = CodeHasher.file_hashes_for(simple_func)
+        assert isinstance(result, dict)
+        assert os.path.basename(__file__) in result
+
+    def test_values_are_sha256_hex(self):
+        result = CodeHasher.file_hashes_for(simple_func)
+        for name, digest in result.items():
+            assert len(digest) == 64, f"{name} hash is not SHA-256 hex"
+            int(digest, 16)  # valid hex
+
+    def test_builtin_returns_empty_dict(self):
+        assert CodeHasher.file_hashes_for(len) == {}
+
+    def test_transitive_import_included(self, tmp_path, monkeypatch):
+        (tmp_path / "pyproject.toml").write_text("[project]\nname='test'\n")
+        helper = tmp_path / "helper_fh.py"
+        helper.write_text("def assist(x):\n    return x\n")
+        main = tmp_path / "main_fh.py"
+        main.write_text(
+            "import helper_fh\n\ndef process(x):\n    return helper_fh.assist(x)\n"
+        )
+
+        import importlib.util  # noqa: PLC0415
+
+        # Add tmp_path to sys.path so the dynamic import can resolve helper_fh
+        monkeypatch.syspath_prepend(str(tmp_path))
+
+        spec = importlib.util.spec_from_file_location("main_fh", str(main))
+        mod = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(mod)
+
+        result = CodeHasher.file_hashes_for(mod.process)
+        assert "main_fh.py" in result
+        assert "helper_fh.py" in result
+
+    def test_no_extra_parse_after_hash_function(self):
+        """file_hashes_for reuses caches populated by hash_function."""
+        CodeHasher.hash_function(simple_func)
+        count_after_hash = CodeHasher._hash_call_count
+
+        CodeHasher.file_hashes_for(simple_func)
+        # No additional parsing should occur
+        assert CodeHasher._hash_call_count == count_after_hash
diff --git a/tests/cache/test_filesystem_backend.py b/tests/cache/test_filesystem_backend.py
new file mode 100644
index 00000000..c575bff9
--- /dev/null
+++ b/tests/cache/test_filesystem_backend.py
@@ -0,0 +1,238 @@
+"""Tests for FilesystemBackend registry operations including file locking and atomic_update."""
+
+from __future__ import annotations
+
+import json
+import threading
+
+import pytest
+
+from kedro.cache.backends.filesystem_backend import FilesystemBackend
+from kedro.cache.step_cache import StepCache
+
+
+@pytest.fixture
+def registry(tmp_path):
+    return FilesystemBackend(path=str(tmp_path / "cache.json"))
+
+
+@pytest.fixture
+def sample_step_cache():
+    return StepCache(
+        step_id="node_a",
+        start_timestamp="2024-01-01T00:00:00+00:00",
+        end_timestamp="2024-01-01T00:00:01+00:00",
+        session_id="sess-1",
+        worker_id="main",
+        code_hash="abc",
+        input_hash="def",
+        parameter_hash="ghi",
+    )
+
+
+class TestFilesystemBackendSetGet:
+    def test_get_returns_none_for_missing_key(self, registry):
+        assert registry.get("nonexistent") is None
+
+    def test_set_and_get_roundtrip(self, registry, sample_step_cache):
+        registry.set("node_a", sample_step_cache)
+        result = registry.get("node_a")
+        assert result is not None
+        assert result.step_id == "node_a"
+        assert result.code_hash == "abc"
+
+    def test_overwrite_existing_entry(self, registry, sample_step_cache):
+        registry.set("node_a", sample_step_cache)
+        updated = StepCache(
+            **{**sample_step_cache.to_dict(), "code_hash": "updated_hash"}
+        )
+        registry.set("node_a", updated)
+        result = registry.get("node_a")
+        assert result.code_hash == "updated_hash"
+
+
+class TestFilesystemBackendDelete:
+    def test_delete_removes_entry(self, registry, sample_step_cache):
+        registry.set("node_a", sample_step_cache)
+        registry.delete("node_a")
+        assert registry.get("node_a") is None
+
+    def test_delete_nonexistent_key_is_noop(self, registry):
+        # Should not raise
+        registry.delete("does_not_exist")
+
+
+class TestFilesystemBackendPendingSentinel:
+    def test_pending_sentinel_not_returned_by_get(self, registry):
+        sentinel = "node_a.__pending__"
+        placeholder = StepCache(
+            step_id=sentinel,
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+        )
+        registry.set(sentinel, placeholder)
+        assert registry.get(sentinel) is None
+
+    def test_set_pending_creates_entry(self, registry):
+        registry.set_pending("node_a.__pending__")
+        # Load raw store to verify sentinel exists
+        store = registry._load_store()
+        assert "node_a.__pending__" in store
+
+    def test_delete_pending_removes_sentinel(self, registry):
+        registry.set_pending("node_a.__pending__")
+        registry.delete("node_a.__pending__")
+        store = registry._load_store()
+        assert "node_a.__pending__" not in store
+
+
+class TestFilesystemBackendCorruptFile:
+    def test_corrupt_json_file_returns_empty(self, tmp_path):
+        path = tmp_path / "corrupt.json"
+        path.write_text("this is not valid json {{{")
+        registry = FilesystemBackend(path=str(path))
+        assert registry.get("any_key") is None
+
+    def test_missing_file_returns_none(self, tmp_path):
+        registry = FilesystemBackend(path=str(tmp_path / "nonexistent.json"))
+        assert registry.get("key") is None
+
+
+class TestFilesystemBackendAtomicWrite:
+    def test_data_persists_to_disk(self, tmp_path, sample_step_cache):
+        path = tmp_path / "cache.json"
+        registry = FilesystemBackend(path=str(path))
+        registry.set("node_a", sample_step_cache)
+
+        # Read raw file and verify content
+        with open(path) as f:
+            data = json.load(f)
+        assert "node_a" in data
+        assert data["node_a"]["step_id"] == "node_a"
+
+
+class TestFilesystemBackendAtomicUpdate:
+    def test_atomic_update_increments_field(self, registry, sample_step_cache):
+        registry.set("node_a", sample_step_cache)
+
+        def _inc(existing):
+            if existing is None:
+                return None
+            return StepCache(
+                **{**existing.to_dict(), "cache_hits": existing.cache_hits + 1}
+            )
+
+        registry.atomic_update("node_a", _inc)
+        result = registry.get("node_a")
+        assert result.cache_hits == 1
+
+    def test_atomic_update_deletes_when_updater_returns_none(
+        self, registry, sample_step_cache
+    ):
+        registry.set("node_a", sample_step_cache)
+
+        registry.atomic_update("node_a", lambda _: None)
+        assert registry.get("node_a") is None
+
+    def test_atomic_update_on_missing_key(self, registry):
+        called_with = []
+
+        def _updater(existing):
+            called_with.append(existing)
+
+        registry.atomic_update("missing", _updater)
+        assert called_with == [None]
+
+
+class TestFilesystemBackendConcurrency:
+    """Verify that concurrent threads do not lose writes."""
+
+    def test_concurrent_sets_all_persisted(self, tmp_path):
+        path = str(tmp_path / "concurrent.json")
+        registry = FilesystemBackend(path=path)
+        errors: list[Exception] = []
+        n_threads = 20
+
+        def _writer(idx):
+            try:
+                sc = StepCache(
+                    step_id=f"node_{idx}",
+                    start_timestamp="",
+                    end_timestamp="",
+                    session_id="",
+                    worker_id="",
+                    cache_hits=idx,
+                )
+                registry.set(f"node_{idx}", sc)
+            except Exception as exc:
+                errors.append(exc)
+
+        threads = [
+            threading.Thread(target=_writer, args=(i,)) for i in range(n_threads)
+        ]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        assert errors == []
+        # All entries must be present
+        for i in range(n_threads):
+            entry = registry.get(f"node_{i}")
+            assert entry is not None, f"node_{i} missing after concurrent set"
+            assert entry.cache_hits == i
+
+    def test_concurrent_atomic_updates_no_lost_increments(self, tmp_path):
+        path = str(tmp_path / "atomic.json")
+        registry = FilesystemBackend(path=path)
+        initial = StepCache(
+            step_id="shared",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            cache_hits=0,
+        )
+        registry.set("shared", initial)
+
+        n_threads = 50
+        errors: list[Exception] = []
+
+        def _increment():
+            try:
+
+                def _inc(existing):
+                    if existing is None:
+                        return None
+                    return StepCache(
+                        **{**existing.to_dict(), "cache_hits": existing.cache_hits + 1}
+                    )
+
+                registry.atomic_update("shared", _inc)
+            except Exception as exc:
+                errors.append(exc)
+
+        threads = [threading.Thread(target=_increment) for _ in range(n_threads)]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        assert errors == []
+        result = registry.get("shared")
+        assert result.cache_hits == n_threads
+
+
+# ---------------------------------------------------------------------------
+# Factory integration
+# ---------------------------------------------------------------------------
+
+
+class TestCreateRegistryFilesystem:
+    def test_create_registry_filesystem_type(self, tmp_path):
+        from kedro.cache.registry import create_registry  # noqa: PLC0415
+
+        reg = create_registry("filesystem", path=str(tmp_path / "cache.json"))
+        assert isinstance(reg, FilesystemBackend)
diff --git a/tests/cache/test_hook.py b/tests/cache/test_hook.py
new file mode 100644
index 00000000..d3aaa352
--- /dev/null
+++ b/tests/cache/test_hook.py
@@ -0,0 +1,556 @@
+"""Tests for CachingHook lifecycle and dirty-node propagation."""
+
+from __future__ import annotations
+
+import os
+
+import pytest
+
+from kedro.cache.hook import CachingHook, _resolve_output_path, cache_transaction
+from kedro.cache.step_cache import StepCache
+from kedro.io import DataCatalog, MemoryDataset
+from kedro.pipeline import Pipeline, node
+
+# ---------------------------------------------------------------------------
+# Helper step functions (module-level for inspect)
+# ---------------------------------------------------------------------------
+
+
+def step_a(x):
+    return {"y": x + 1}
+
+
+def step_b(y):
+    return {"z": y * 2}
+
+
+def step_c(z):
+    return {"result": z + 10}
+
+
+# ---------------------------------------------------------------------------
+# Fixtures
+# ---------------------------------------------------------------------------
+
+
+@pytest.fixture
+def three_node_pipeline():
+    return Pipeline(
+        [
+            node(step_a, inputs="x", outputs="y", name="step_a"),
+            node(step_b, inputs="y", outputs="z", name="step_b"),
+            node(step_c, inputs="z", outputs="result", name="step_c"),
+        ]
+    )
+
+
+@pytest.fixture
+def catalog():
+    return DataCatalog(
+        datasets={
+            "x": MemoryDataset(data=5),
+            "y": MemoryDataset(),
+            "z": MemoryDataset(),
+            "result": MemoryDataset(),
+        }
+    )
+
+
+@pytest.fixture
+def hook(registry_fixture):
+    return CachingHook(registry=registry_fixture)
+
+
+# ---------------------------------------------------------------------------
+# Tests: before_pipeline_run
+# ---------------------------------------------------------------------------
+
+
+class TestBeforePipelineRun:
+    def test_resets_dirty_nodes(self, hook, three_node_pipeline, catalog):
+        hook._dirty_nodes = {"stale_node"}
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        assert len(hook._dirty_nodes) == 0
+
+    def test_builds_node_children_map(self, hook, three_node_pipeline, catalog):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # step_a produces y, consumed by step_b
+        assert "step_b" in hook._node_children.get("step_a", set())
+        # step_b produces z, consumed by step_c
+        assert "step_c" in hook._node_children.get("step_b", set())
+
+    def test_masks_sensitive_cli_flags(self, hook, three_node_pipeline, catalog):
+        hook.before_pipeline_run(
+            run_params={
+                "runtime_params": {
+                    "api_key": "sk-secret-123",
+                    "password": "hunter2",
+                    "verbose": True,
+                }
+            },
+            pipeline=three_node_pipeline,
+            catalog=catalog,
+        )
+        assert hook._cli_flags["api_key"] == "***REDACTED***"
+        assert hook._cli_flags["password"] == "***REDACTED***"  # noqa: S105
+        assert hook._cli_flags["verbose"] is True
+
+    def test_masked_flags_persisted_in_step_cache(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={
+                "runtime_params": {
+                    "secret_token": "tok-abc",
+                    "env": "prod",
+                }
+            },
+            pipeline=three_node_pipeline,
+            catalog=catalog,
+        )
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        hook.after_node_run(
+            node=pipeline_node,
+            catalog=catalog,
+            inputs={"x": 5},
+            outputs={"y": 6},
+            is_async=False,
+            run_id="run-1",
+        )
+        entry = registry_fixture.get("step_a")
+        assert entry.cli_flags["secret_token"] == "***REDACTED***"  # noqa: S105
+        assert entry.cli_flags["env"] == "prod"
+
+
+# ---------------------------------------------------------------------------
+# Tests: is_cached
+# ---------------------------------------------------------------------------
+
+
+class TestIsCached:
+    def test_returns_false_when_no_entry_exists(
+        self, hook, three_node_pipeline, catalog
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pipeline_node = three_node_pipeline.nodes[0]
+        result = hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert result is False
+
+    def test_returns_false_for_dirty_node(self, hook, three_node_pipeline, catalog):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        hook._dirty_nodes.add("step_b")
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_b")
+        result = hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert result is False
+
+    def test_marks_downstream_dirty_on_miss(self, hook, three_node_pipeline, catalog):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # step_a has no cache entry → miss → step_b and step_c should be dirty
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_a")
+        hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert "step_b" in hook._dirty_nodes
+        assert "step_c" in hook._dirty_nodes
+
+
+# ---------------------------------------------------------------------------
+# Tests: after_node_run
+# ---------------------------------------------------------------------------
+
+
+class TestAfterNodeRun:
+    def test_registers_cache_entry(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_a")
+
+        hook.after_node_run(
+            node=pipeline_node,
+            catalog=catalog,
+            inputs={"x": 5},
+            outputs={"y": 6},
+            is_async=False,
+            run_id="run-1",
+        )
+        entry = registry_fixture.get("step_a")
+        assert entry is not None
+        assert entry.step_id == "step_a"
+
+
+# ---------------------------------------------------------------------------
+# Tests: on_node_error
+# ---------------------------------------------------------------------------
+
+
+class TestOnNodeError:
+    def test_cleans_up_sentinel_on_error(
+        self, hook, registry_fixture, three_node_pipeline, catalog
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        sentinel = "step_a.__pending__"
+        registry_fixture.set_pending(sentinel)
+
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_a")
+        hook.on_node_error(
+            error=RuntimeError("fail"),
+            node=pipeline_node,
+            catalog=catalog,
+            inputs={"x": 5},
+            is_async=False,
+            run_id="run-1",
+        )
+        # Sentinel should be gone
+        store = registry_fixture._load_store()
+        assert sentinel not in store
+
+
+# ---------------------------------------------------------------------------
+# Tests: cache_transaction context manager
+# ---------------------------------------------------------------------------
+
+
+class TestCacheTransaction:
+    def test_successful_transaction_registers_entry(self, registry_fixture):
+        sc = StepCache(
+            step_id="txn_node",
+            start_timestamp="2024-01-01T00:00:00+00:00",
+            end_timestamp="2024-01-01T00:00:01+00:00",
+            session_id="s1",
+            worker_id="main",
+        )
+        with cache_transaction(registry_fixture, sc):
+            pass  # simulates successful save
+
+        assert registry_fixture.get("txn_node") is not None
+
+    def test_failed_transaction_does_not_register(self, registry_fixture):
+        sc = StepCache(
+            step_id="fail_node",
+            start_timestamp="2024-01-01T00:00:00+00:00",
+            end_timestamp="2024-01-01T00:00:01+00:00",
+            session_id="s1",
+            worker_id="main",
+        )
+        with pytest.raises(ValueError, match="save failed"):
+            with cache_transaction(registry_fixture, sc):
+                raise ValueError("save failed")
+
+        assert registry_fixture.get("fail_node") is None
+
+    def test_sentinel_cleaned_up_on_failure(self, registry_fixture):
+        sc = StepCache(
+            step_id="sentinel_node",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+        )
+        with pytest.raises(RuntimeError):
+            with cache_transaction(registry_fixture, sc):
+                raise RuntimeError("boom")
+
+        store = registry_fixture._load_store()
+        assert "sentinel_node.__pending__" not in store
+
+
+# ---------------------------------------------------------------------------
+# Tests: increment_cache_hit
+# ---------------------------------------------------------------------------
+
+
+class TestIncrementCacheHit:
+    def test_increments_counter(self, hook, registry_fixture):
+        sc = StepCache(
+            step_id="hit_node",
+            start_timestamp="2024-01-01T00:00:00+00:00",
+            end_timestamp="2024-01-01T00:00:01+00:00",
+            session_id="s1",
+            worker_id="main",
+            cache_hits=0,
+        )
+        registry_fixture.set("hit_node", sc)
+        hook.increment_cache_hit("hit_node")
+
+        updated = registry_fixture.get("hit_node")
+        assert updated.cache_hits == 1
+
+    def test_no_op_for_missing_entry(self, hook):
+        # Should not raise
+        hook.increment_cache_hit("nonexistent_node")
+
+
+# ---------------------------------------------------------------------------
+# Tests: _build_current_cache fallback for missing inputs
+# ---------------------------------------------------------------------------
+
+
+class TestBuildCurrentCacheFallback:
+    """When upstream outputs are unavailable (skipped node + dataset released),
+    _build_current_cache should fall back to the stored hashes so the
+    comparison stays valid instead of producing a spurious miss."""
+
+    def test_missing_input_reuses_stored_input_hash(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # Register a cache entry for step_b with a known input_hash
+        stored = StepCache(
+            step_id="step_b",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            code_hash="abc",
+            input_hash="stored_input_hash",
+            parameter_hash="stored_param_hash",
+        )
+        registry_fixture.set("step_b", stored)
+
+        # step_b's input "y" is an empty MemoryDataset — load returns None
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_b")
+        result = hook._build_current_cache(
+            pipeline_node, catalog, run_id="run-1", stored=stored
+        )
+        # Should have fallen back to the stored input_hash
+        assert result.input_hash == "stored_input_hash"
+
+    def test_available_inputs_compute_fresh_hash(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        stored = StepCache(
+            step_id="step_a",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            code_hash="abc",
+            input_hash="old_hash",
+            parameter_hash="old_param",
+        )
+        registry_fixture.set("step_a", stored)
+
+        # step_a's input "x" has data (MemoryDataset(data=5))
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_a")
+        result = hook._build_current_cache(
+            pipeline_node, catalog, run_id="run-1", stored=stored
+        )
+        # All inputs available — should compute a fresh hash, not reuse stored
+        assert result.input_hash != "old_hash"
+
+    def test_no_stored_entry_computes_hash_even_with_none_inputs(
+        self, hook, three_node_pipeline, catalog
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # step_b input "y" is empty, but stored=None so no fallback available
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "step_b")
+        result = hook._build_current_cache(
+            pipeline_node, catalog, run_id="run-1", stored=None
+        )
+        # Must still return a StepCache with a computed hash (even if input was None)
+        assert isinstance(result.input_hash, str)
+        assert len(result.input_hash) == 64  # SHA-256 hex
+
+
+# ---------------------------------------------------------------------------
+# Tests: output existence verification on cache check
+# ---------------------------------------------------------------------------
+
+
+class TestOutputExistenceCheck:
+    def test_missing_output_file_causes_miss(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # Register a cache entry with a non-existent output path
+        stored = StepCache(
+            step_id="step_a",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            code_hash="abc",
+            input_hash="def",
+            parameter_hash="ghi",
+            output_paths={"y": "/nonexistent/path/output.parquet"},
+        )
+        registry_fixture.set("step_a", stored)
+
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        result = hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert result is False
+
+    def test_existing_output_file_does_not_block(
+        self, hook, three_node_pipeline, catalog, registry_fixture, tmp_path
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # Create a real file so the existence check passes
+        real_file = tmp_path / "output.parquet"
+        real_file.write_text("data")
+
+        # Build a current-matching cache entry with the real output path
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        current = hook._build_current_cache(
+            pipeline_node, catalog, run_id="run-1", stored=None
+        )
+        stored = StepCache(
+            **{
+                **current.to_dict(),
+                "output_paths": {"y": str(real_file)},
+            }
+        )
+        registry_fixture.set("step_a", stored)
+
+        # is_cached should not fail on the existence check and proceed to
+        # field comparison (which will match since we used the current snapshot)
+        result = hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert result is True
+
+    def test_empty_output_path_skipped(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        current = hook._build_current_cache(
+            pipeline_node, catalog, run_id="run-1", stored=None
+        )
+        # Empty path = non-file dataset; should not trigger a miss
+        stored = StepCache(
+            **{**current.to_dict(), "output_paths": {"y": ""}}
+        )
+        registry_fixture.set("step_a", stored)
+
+        result = hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert result is True
+
+    def test_missing_output_marks_downstream_dirty(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        stored = StepCache(
+            step_id="step_a",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            output_paths={"y": "/gone/output.parquet"},
+        )
+        registry_fixture.set("step_a", stored)
+
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        hook.is_cached(pipeline_node, catalog, run_id="run-1")
+        assert "step_b" in hook._dirty_nodes
+        assert "step_c" in hook._dirty_nodes
+
+
+# ---------------------------------------------------------------------------
+# Tests: code_files populated in StepCache
+# ---------------------------------------------------------------------------
+
+
+class TestCodeFilesPopulation:
+    def test_build_current_cache_includes_code_files(
+        self, hook, three_node_pipeline, catalog
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        result = hook._build_current_cache(
+            pipeline_node, catalog, run_id="run-1", stored=None
+        )
+        # code_files should be a non-empty dict with hex SHA-256 values
+        assert isinstance(result.code_files, dict)
+        assert len(result.code_files) > 0
+        for name, digest in result.code_files.items():
+            assert digest.endswith(digest)  # sanity: is a string
+            assert len(digest) == 64
+
+    def test_after_node_run_stores_code_files(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pipeline_node = next(
+            n for n in three_node_pipeline.nodes if n.name == "step_a"
+        )
+        hook.after_node_run(
+            node=pipeline_node,
+            catalog=catalog,
+            inputs={"x": 5},
+            outputs={"y": 6},
+            is_async=False,
+            run_id="run-1",
+        )
+        entry = registry_fixture.get("step_a")
+        assert entry is not None
+        assert isinstance(entry.code_files, dict)
+        assert len(entry.code_files) > 0
+
+
+# ---------------------------------------------------------------------------
+# Tests: _resolve_output_path helper
+# ---------------------------------------------------------------------------
+
+
+class TestResolveOutputPath:
+    def test_memory_dataset_returns_empty(self):
+        catalog = DataCatalog(datasets={"x": MemoryDataset(data=1)})
+        assert _resolve_output_path(catalog, "x") == ""
+
+    def test_missing_dataset_returns_empty(self):
+        catalog = DataCatalog(datasets={})
+        assert _resolve_output_path(catalog, "missing") == ""
+
+    def test_dataset_with_filepath_attr(self, tmp_path):
+        """A dataset exposing a filepath attribute should resolve."""
+
+        class _FakeDataset:
+            def __init__(self, path):
+                self._filepath = path
+
+        catalog = DataCatalog(datasets={})
+        catalog._datasets = {"out": _FakeDataset(str(tmp_path / "out.csv"))}
+        result = _resolve_output_path(catalog, "out")
+        assert result == str(os.path.abspath(tmp_path / "out.csv"))
diff --git a/tests/cache/test_locks.py b/tests/cache/test_locks.py
new file mode 100644
index 00000000..e84e63cc
--- /dev/null
+++ b/tests/cache/test_locks.py
@@ -0,0 +1,108 @@
+"""Direct unit tests for the file_lock context manager."""
+
+from __future__ import annotations
+
+import os
+import threading
+import time
+
+from kedro.cache._locks import file_lock
+
+
+class TestFileLockBasic:
+    def test_lock_creates_lock_file(self, tmp_path):
+        lock_path = str(tmp_path / "test.lock")
+        with file_lock(lock_path):
+            assert os.path.exists(lock_path)
+
+    def test_lock_and_unlock_cycle(self, tmp_path):
+        lock_path = str(tmp_path / "test.lock")
+        # Two sequential acquisitions must not deadlock
+        with file_lock(lock_path):
+            pass
+        with file_lock(lock_path):
+            pass
+
+    def test_stale_lock_file_does_not_block(self, tmp_path):
+        """A leftover .lock file from a previous (dead) process must not
+        prevent acquisition — advisory locks are per-fd, not per-file."""
+        lock_path = str(tmp_path / "stale.lock")
+        # Create a stale lock file (simulates a crashed process)
+        with open(lock_path, "w") as f:
+            f.write("")
+        # Must acquire immediately without hanging
+        with file_lock(lock_path):
+            pass
+
+
+class TestFileLockContention:
+    def test_two_threads_serialised(self, tmp_path):
+        """Two threads competing for the same lock must execute their
+        critical sections sequentially — no interleaving."""
+        lock_path = str(tmp_path / "contention.lock")
+        order: list[str] = []
+
+        def _worker(label: str) -> None:
+            with file_lock(lock_path):
+                order.append(f"{label}-start")
+                time.sleep(0.05)  # hold long enough for the other thread to arrive
+                order.append(f"{label}-end")
+
+        t1 = threading.Thread(target=_worker, args=("A",))
+        t2 = threading.Thread(target=_worker, args=("B",))
+        t1.start()
+        time.sleep(0.01)  # give t1 a head start to grab the lock first
+        t2.start()
+        t1.join()
+        t2.join()
+
+        # One thread must fully finish before the other starts
+        assert order in [
+            ["A-start", "A-end", "B-start", "B-end"],
+            ["B-start", "B-end", "A-start", "A-end"],
+        ]
+
+    def test_many_threads_no_corruption(self, tmp_path):
+        """30 threads each append a value under lock — final list must have
+        exactly 30 entries with no duplicates or missing writes."""
+        lock_path = str(tmp_path / "many.lock")
+        shared_file = str(tmp_path / "shared.txt")
+        # Initialise
+        with open(shared_file, "w") as f:
+            f.write("")
+
+        n_threads = 30
+
+        def _writer(idx: int) -> None:
+            with file_lock(lock_path):
+                with open(shared_file, "a") as f:
+                    f.write(f"{idx}\n")
+
+        threads = [
+            threading.Thread(target=_writer, args=(i,)) for i in range(n_threads)
+        ]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        with open(shared_file) as f:
+            lines = [line.strip() for line in f if line.strip()]
+
+        assert len(lines) == n_threads
+        assert set(lines) == {str(i) for i in range(n_threads)}
+
+    def test_exception_inside_lock_releases_it(self, tmp_path):
+        """If the body raises, the lock must still be released so a
+        subsequent acquisition succeeds."""
+        lock_path = str(tmp_path / "exception.lock")
+
+        try:
+            with file_lock(lock_path):
+                raise RuntimeError("intentional")
+        except RuntimeError:
+            pass
+
+        # Must not hang — lock was released
+        with file_lock(lock_path):
+            pass
diff --git a/tests/cache/test_redis_backend.py b/tests/cache/test_redis_backend.py
new file mode 100644
index 00000000..d6ca9084
--- /dev/null
+++ b/tests/cache/test_redis_backend.py
@@ -0,0 +1,390 @@
+"""Tests for RedisBackend using an in-memory fake Redis client."""
+
+from __future__ import annotations
+
+import json
+import threading
+from unittest.mock import patch
+
+import pytest
+
+from kedro.cache.backends.redis_backend import RedisBackend
+from kedro.cache.step_cache import StepCache
+
+# ---------------------------------------------------------------------------
+# Fake Redis client — simulates HGET/HSET/HDEL + Lua script semantics
+# ---------------------------------------------------------------------------
+
+
+class _FakeScript:
+    """Mimics the callable returned by redis.Redis.register_script."""
+
+    def __init__(self, lua_source: str, client: _FakeRedisClient) -> None:
+        self._lua = lua_source
+        self._client = client
+
+    def __call__(self, keys: list[str], args: list[str] | None = None) -> int:
+        """Execute the Lua script against the fake store under lock."""
+        with self._client._lock:
+            if (
+                "cache_hits" in self._lua
+                and "HGET" in self._lua
+                and "HSET" in self._lua
+                and "cjson" in self._lua
+            ):
+                # _LUA_INCREMENT_HITS
+                return self._execute_increment_hits(keys)
+            # _LUA_COMPARE_AND_SWAP
+            return self._execute_cas(keys, args)
+
+    def _execute_increment_hits(self, keys: list[str]) -> int:
+        hash_key, field = keys[0], keys[1]
+        store = self._client._data.get(hash_key, {})
+        raw = store.get(field)
+        if raw is None:
+            return 0
+        data = json.loads(raw)
+        data["cache_hits"] = data.get("cache_hits", 0) + 1
+        store[field] = json.dumps(data)
+        self._client._data[hash_key] = store
+        return 1
+
+    def _execute_cas(self, keys: list[str], args: list[str] | None) -> int:
+        if args is None:
+            return 0
+        hash_key, field = keys[0], keys[1]
+        expected, new_val = args[0], args[1]
+        store = self._client._data.get(hash_key, {})
+        current = store.get(field)
+
+        # Compare phase
+        if expected != "" and str(current) != expected:
+            return 0
+
+        # Swap phase
+        if new_val == "":
+            store.pop(field, None)
+        else:
+            store[field] = new_val
+        self._client._data[hash_key] = store
+        return 1
+
+
+class _FakeRedisClient:
+    """Thread-safe in-memory Redis hash store."""
+
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        # hash_key -> { field -> json_string }
+        self._data: dict[str, dict[str, str]] = {}
+
+    def hget(self, name: str, key: str) -> str | None:
+        with self._lock:
+            return self._data.get(name, {}).get(key)
+
+    def hset(self, name: str, key: str, value: str) -> None:
+        with self._lock:
+            self._data.setdefault(name, {})[key] = value
+
+    def hdel(self, name: str, *keys: str) -> None:
+        with self._lock:
+            store = self._data.get(name, {})
+            for k in keys:
+                store.pop(k, None)
+
+    def register_script(self, script: str) -> _FakeScript:
+        return _FakeScript(script, self)
+
+
+# ---------------------------------------------------------------------------
+# Fixtures
+# ---------------------------------------------------------------------------
+
+
+@pytest.fixture
+def fake_client():
+    return _FakeRedisClient()
+
+
+@pytest.fixture
+def backend(fake_client):
+    """RedisBackend wired to the fake client (bypasses real redis.Redis)."""
+    with patch("redis.Redis") as mock_redis_cls:
+        mock_redis_cls.return_value = fake_client
+        b = RedisBackend(host="localhost", port=6379, db=0)
+    # Swap in our fake after construction so register_script results persist
+    b._client = fake_client
+    b._increment_hits_script = fake_client.register_script("")  # re-register with fake
+    b._cas_script = fake_client.register_script("")
+    # Re-register with actual script sources so the fake can dispatch correctly
+    from kedro.cache.backends.redis_backend import (  # noqa: PLC0415
+        _LUA_COMPARE_AND_SWAP,
+        _LUA_INCREMENT_HITS,
+    )
+
+    b._increment_hits_script = fake_client.register_script(_LUA_INCREMENT_HITS)
+    b._cas_script = fake_client.register_script(_LUA_COMPARE_AND_SWAP)
+    return b
+
+
+@pytest.fixture
+def sample_cache():
+    return StepCache(
+        step_id="redis_node",
+        start_timestamp="2024-01-01T00:00:00+00:00",
+        end_timestamp="2024-01-01T00:00:01+00:00",
+        session_id="sess-redis",
+        worker_id="main",
+        code_hash="redishash",
+        input_hash="inp",
+        parameter_hash="par",
+    )
+
+
+# ---------------------------------------------------------------------------
+# Basic CRUD
+# ---------------------------------------------------------------------------
+
+
+class TestRedisBackendGetSet:
+    def test_get_returns_none_on_empty(self, backend):
+        assert backend.get("missing") is None
+
+    def test_set_and_get_roundtrip(self, backend, sample_cache):
+        backend.set("redis_node", sample_cache)
+        result = backend.get("redis_node")
+        assert result is not None
+        assert result.step_id == "redis_node"
+        assert result.code_hash == "redishash"
+
+    def test_overwrite_entry(self, backend, sample_cache):
+        backend.set("redis_node", sample_cache)
+        updated = StepCache(**{**sample_cache.to_dict(), "code_hash": "new_redis_hash"})
+        backend.set("redis_node", updated)
+        result = backend.get("redis_node")
+        assert result.code_hash == "new_redis_hash"
+
+
+class TestRedisBackendDelete:
+    def test_delete_removes_entry(self, backend, sample_cache):
+        backend.set("redis_node", sample_cache)
+        backend.delete("redis_node")
+        assert backend.get("redis_node") is None
+
+    def test_delete_nonexistent_is_noop(self, backend):
+        backend.delete("does_not_exist")  # must not raise
+
+
+class TestRedisBackendPendingSentinel:
+    def test_pending_sentinel_hidden_from_get(self, backend):
+        backend.set_pending("node.__pending__")
+        assert backend.get("node.__pending__") is None
+
+    def test_set_pending_writes_to_store(self, backend, fake_client):
+        backend.set_pending("node.__pending__")
+        raw = fake_client.hget("kedro:step_cache", "node.__pending__")
+        assert raw is not None
+        assert json.loads(raw) == {}
+
+
+class TestRedisBackendCorruptEntry:
+    def test_corrupt_json_returns_none(self, backend, fake_client):
+        fake_client.hset("kedro:step_cache", "bad_node", "not-valid-json{{{")
+        assert backend.get("bad_node") is None
+
+    def test_missing_fields_returns_none(self, backend, fake_client):
+        fake_client.hset("kedro:step_cache", "partial", json.dumps({"step_id": "x"}))
+        assert backend.get("partial") is None
+
+
+# ---------------------------------------------------------------------------
+# atomic_update with CAS
+# ---------------------------------------------------------------------------
+
+
+class TestRedisBackendAtomicUpdate:
+    def test_atomic_update_increments_field(self, backend, sample_cache):
+        backend.set("redis_node", sample_cache)
+
+        def _inc(existing):
+            if existing is None:
+                return None
+            return StepCache(
+                **{**existing.to_dict(), "cache_hits": existing.cache_hits + 1}
+            )
+
+        backend.atomic_update("redis_node", _inc)
+        result = backend.get("redis_node")
+        assert result.cache_hits == 1
+
+    def test_atomic_update_deletes_when_updater_returns_none(
+        self, backend, sample_cache
+    ):
+        backend.set("redis_node", sample_cache)
+        backend.atomic_update("redis_node", lambda _: None)
+        assert backend.get("redis_node") is None
+
+    def test_atomic_update_on_missing_key_passes_none(self, backend):
+        received = []
+
+        def _updater(existing):
+            received.append(existing)
+
+        backend.atomic_update("missing", _updater)
+        assert received == [None]
+
+    def test_atomic_update_retries_on_cas_conflict(self, backend, fake_client):
+        """Simulate a concurrent writer that changes the field between the
+        backend's read and CAS write.  The backend should retry and succeed."""
+        initial = StepCache(
+            step_id="contested",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            cache_hits=0,
+        )
+        backend.set("contested", initial)
+
+        # Intercept the CAS script to inject one conflict
+        original_cas = backend._cas_script
+        conflict_count = {"n": 0}
+
+        class _ConflictingCAS:
+            def __call__(self_, keys, args):
+                if args[0] != "" and conflict_count["n"] < 1:
+                    conflict_count["n"] += 1
+                    # Silently bump the stored value to simulate concurrent write
+                    store = json.loads(
+                        fake_client.hget("kedro:step_cache", "contested")
+                    )
+                    store["cache_hits"] = 999
+                    fake_client.hset(
+                        "kedro:step_cache",
+                        "contested",
+                        json.dumps(store),
+                    )
+                    return 0  # CAS failure
+                return original_cas(keys=keys, args=args)
+
+        backend._cas_script = _ConflictingCAS()
+
+        def _inc(existing):
+            if existing is None:
+                return None
+            return StepCache(
+                **{**existing.to_dict(), "cache_hits": existing.cache_hits + 1}
+            )
+
+        backend.atomic_update("contested", _inc)
+        result = backend.get("contested")
+        # After retry it read the noise value (999) and incremented
+        assert result.cache_hits == 1000
+
+
+# ---------------------------------------------------------------------------
+# atomic_increment_cache_hits (dedicated Lua script path)
+# ---------------------------------------------------------------------------
+
+
+class TestRedisBackendIncrementHits:
+    def test_increment_hits_returns_true_on_existing(self, backend, sample_cache):
+        backend.set("redis_node", sample_cache)
+        assert backend.atomic_increment_cache_hits("redis_node") is True
+        result = backend.get("redis_node")
+        assert result.cache_hits == 1
+
+    def test_increment_hits_idempotent_multiple_calls(self, backend, sample_cache):
+        backend.set("redis_node", sample_cache)
+        backend.atomic_increment_cache_hits("redis_node")
+        backend.atomic_increment_cache_hits("redis_node")
+        backend.atomic_increment_cache_hits("redis_node")
+        result = backend.get("redis_node")
+        assert result.cache_hits == 3
+
+    def test_increment_hits_returns_false_on_missing(self, backend):
+        assert backend.atomic_increment_cache_hits("nonexistent") is False
+
+
+# ---------------------------------------------------------------------------
+# Concurrency — multiple threads hammering atomic_update
+# ---------------------------------------------------------------------------
+
+
+class TestRedisBackendConcurrency:
+    def test_concurrent_atomic_updates_no_lost_increments(self, backend, sample_cache):
+        initial = StepCache(
+            **{**sample_cache.to_dict(), "step_id": "shared", "cache_hits": 0}
+        )
+        backend.set("shared", initial)
+
+        n_threads = 30
+        errors: list[Exception] = []
+
+        def _increment():
+            try:
+
+                def _inc(existing):
+                    if existing is None:
+                        return None
+                    return StepCache(
+                        **{
+                            **existing.to_dict(),
+                            "cache_hits": existing.cache_hits + 1,
+                        }
+                    )
+
+                backend.atomic_update("shared", _inc)
+            except Exception as exc:
+                errors.append(exc)
+
+        threads = [threading.Thread(target=_increment) for _ in range(n_threads)]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        assert errors == []
+        result = backend.get("shared")
+        assert result.cache_hits == n_threads
+
+    def test_concurrent_increment_hits_no_lost_increments(self, backend, sample_cache):
+        initial = StepCache(
+            **{**sample_cache.to_dict(), "step_id": "shared2", "cache_hits": 0}
+        )
+        backend.set("shared2", initial)
+
+        n_threads = 30
+        errors: list[Exception] = []
+
+        def _increment():
+            try:
+                backend.atomic_increment_cache_hits("shared2")
+            except Exception as exc:
+                errors.append(exc)
+
+        threads = [threading.Thread(target=_increment) for _ in range(n_threads)]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        assert errors == []
+        result = backend.get("shared2")
+        assert result.cache_hits == n_threads
+
+
+# ---------------------------------------------------------------------------
+# Factory integration
+# ---------------------------------------------------------------------------
+
+
+class TestCreateRegistryRedis:
+    def test_create_registry_redis_type(self):
+        with patch("redis.Redis") as mock_redis_cls:
+            mock_redis_cls.return_value = _FakeRedisClient()
+
+            from kedro.cache.registry import create_registry  # noqa: PLC0415
+
+            reg = create_registry("redis", host="localhost")
+        assert isinstance(reg, RedisBackend)
diff --git a/tests/cache/test_runner_cache.py b/tests/cache/test_runner_cache.py
new file mode 100644
index 00000000..078b0e1e
--- /dev/null
+++ b/tests/cache/test_runner_cache.py
@@ -0,0 +1,210 @@
+"""Unit tests for the runner-side cache integration helpers.
+
+Covers:
+- _extract_caching_hook — finds CachingHook in a PluginManager
+- _is_node_cached — gate logic, dirty-node propagation, increment_cache_hit call
+"""
+
+from __future__ import annotations
+
+from unittest.mock import MagicMock
+
+import pytest
+
+from kedro.cache.hook import CachingHook
+from kedro.io import DataCatalog, MemoryDataset
+from kedro.pipeline import Pipeline, node
+from kedro.runner.runner import (
+    _extract_caching_hook,
+    _is_node_cached,
+)
+
+# ---------------------------------------------------------------------------
+# Helper step functions
+# ---------------------------------------------------------------------------
+
+
+def fn_a(x):
+    return x + 1
+
+
+def fn_b(y):
+    return y * 2
+
+
+def fn_c(z):
+    return z + 10
+
+
+# ---------------------------------------------------------------------------
+# Fixtures
+# ---------------------------------------------------------------------------
+
+
+@pytest.fixture
+def three_node_pipeline():
+    return Pipeline(
+        [
+            node(fn_a, inputs="x", outputs="y", name="node_a"),
+            node(fn_b, inputs="y", outputs="z", name="node_b"),
+            node(fn_c, inputs="z", outputs="result", name="node_c"),
+        ]
+    )
+
+
+@pytest.fixture
+def catalog():
+    return DataCatalog(
+        datasets={
+            "x": MemoryDataset(data=42),
+            "y": MemoryDataset(),
+            "z": MemoryDataset(),
+            "result": MemoryDataset(),
+        }
+    )
+
+
+@pytest.fixture
+def hook(registry_fixture):
+    return CachingHook(registry=registry_fixture)
+
+
+def _make_plugin_manager(hook_instance: CachingHook | None) -> MagicMock:
+    """Build a minimal PluginManager mock that yields the hook (or nothing)."""
+    pm = MagicMock()
+    plugins = [hook_instance] if hook_instance else []
+    pm.get_plugins.return_value = plugins
+    return pm
+
+
+# ---------------------------------------------------------------------------
+# _extract_caching_hook
+# ---------------------------------------------------------------------------
+
+
+class TestExtractCachingHook:
+    def test_returns_hook_when_registered(self, hook):
+        pm = _make_plugin_manager(hook)
+        assert _extract_caching_hook(pm) is hook
+
+    def test_returns_none_when_no_hook_registered(self):
+        pm = _make_plugin_manager(None)
+        assert _extract_caching_hook(pm) is None
+
+    def test_returns_none_when_manager_is_none(self):
+        assert _extract_caching_hook(None) is None
+
+    def test_ignores_non_caching_plugins(self):
+        pm = MagicMock()
+        pm.get_plugins.return_value = [MagicMock(), "not_a_hook", 42]
+        assert _extract_caching_hook(pm) is None
+
+
+# ---------------------------------------------------------------------------
+# _is_node_cached — no CachingHook registered
+# ---------------------------------------------------------------------------
+
+
+class TestIsNodeCachedNoHook:
+    def test_returns_false_when_no_hook(self, three_node_pipeline, catalog):
+        pm = _make_plugin_manager(None)
+        pipeline_node = three_node_pipeline.nodes[0]
+        dirty: set = set()
+        result = _is_node_cached(pipeline_node, catalog, pm, "run-1", dirty)
+        assert result is False
+        assert len(dirty) == 0
+
+    def test_returns_false_when_manager_is_none(self, three_node_pipeline, catalog):
+        pipeline_node = three_node_pipeline.nodes[0]
+        dirty: set = set()
+        result = _is_node_cached(pipeline_node, catalog, None, "run-1", dirty)
+        assert result is False
+
+
+# ---------------------------------------------------------------------------
+# _is_node_cached — cache miss paths
+# ---------------------------------------------------------------------------
+
+
+class TestIsNodeCachedMiss:
+    def test_returns_false_on_no_stored_entry(self, hook, three_node_pipeline, catalog):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pm = _make_plugin_manager(hook)
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "node_a")
+        dirty: set = set()
+        result = _is_node_cached(pipeline_node, catalog, pm, "run-1", dirty)
+        assert result is False
+
+    def test_marks_node_dirty_when_hook_reports_dirty(
+        self, hook, three_node_pipeline, catalog
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # Manually mark node_b as dirty in the hook
+        hook._dirty_nodes.add("node_b")
+        pm = _make_plugin_manager(hook)
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "node_b")
+        dirty: set = set()
+        result = _is_node_cached(pipeline_node, catalog, pm, "run-1", dirty)
+        assert result is False
+        assert pipeline_node in dirty
+
+    def test_downstream_nodes_marked_dirty_on_miss(
+        self, hook, three_node_pipeline, catalog
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pm = _make_plugin_manager(hook)
+        # node_a has no cache entry → miss → node_b and node_c should be dirty
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "node_a")
+        dirty: set = set()
+        _is_node_cached(pipeline_node, catalog, pm, "run-1", dirty)
+        assert "node_b" in hook._dirty_nodes
+        assert "node_c" in hook._dirty_nodes
+
+
+# ---------------------------------------------------------------------------
+# _is_node_cached — cache hit path
+# ---------------------------------------------------------------------------
+
+
+class TestIsNodeCachedHit:
+    def test_returns_true_and_increments_on_hit(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        # Simulate a prior run: register node_a with matching hashes
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "node_a")
+        # Build a cache entry that matches current state
+        current = hook._build_current_cache(pipeline_node, catalog, run_id="run-1")
+        # Store it so the next is_cached check finds a match
+        registry_fixture.set("node_a", current)
+
+        pm = _make_plugin_manager(hook)
+        dirty: set = set()
+        result = _is_node_cached(pipeline_node, catalog, pm, "run-1", dirty)
+        assert result is True
+        # increment_cache_hit should have been called
+        stored = registry_fixture.get("node_a")
+        assert stored.cache_hits == 1
+
+    def test_hit_does_not_add_to_dirty_set(
+        self, hook, three_node_pipeline, catalog, registry_fixture
+    ):
+        hook.before_pipeline_run(
+            run_params={}, pipeline=three_node_pipeline, catalog=catalog
+        )
+        pipeline_node = next(n for n in three_node_pipeline.nodes if n.name == "node_a")
+        current = hook._build_current_cache(pipeline_node, catalog, run_id="run-1")
+        registry_fixture.set("node_a", current)
+
+        pm = _make_plugin_manager(hook)
+        dirty: set = set()
+        _is_node_cached(pipeline_node, catalog, pm, "run-1", dirty)
+        assert len(dirty) == 0
diff --git a/tests/cache/test_s3_backend.py b/tests/cache/test_s3_backend.py
new file mode 100644
index 00000000..f7b145d8
--- /dev/null
+++ b/tests/cache/test_s3_backend.py
@@ -0,0 +1,311 @@
+"""Tests for S3Backend using an in-memory fake S3 client."""
+
+from __future__ import annotations
+
+import json
+import threading
+from unittest.mock import MagicMock, patch
+
+import pytest
+
+from kedro.cache.backends.s3_backend import S3Backend
+from kedro.cache.step_cache import StepCache
+
+# ---------------------------------------------------------------------------
+# Fake S3 client — simulates get_object / put_object with ETag semantics
+# ---------------------------------------------------------------------------
+
+
+class _PreconditionFailed(Exception):
+    """Mimics botocore.exceptions.ClientError for a 412 response."""
+
+    def __init__(self) -> None:
+        super().__init__("PreconditionFailed")
+        self.response = {"Error": {"Code": "PreconditionFailed", "Message": ""}}
+
+
+class _NoSuchKey(Exception):
+    """Mimics botocore ClientError for a missing object."""
+
+    def __init__(self) -> None:
+        super().__init__("NoSuchKey")
+        self.response = {"Error": {"Code": "NoSuchKey", "Message": ""}}
+
+
+class _FakeS3Client:
+    """Thread-safe in-memory S3 object store with ETag conditional-PUT."""
+
+    def __init__(self) -> None:
+        self._lock = threading.Lock()
+        # (bucket, key) -> (json_body: str, etag: str)
+        self._objects: dict[tuple[str, str], tuple[str, str]] = {}
+        self._etag_seq = 0
+
+        # Attach exception classes so S3Backend can duck-type them
+        self.exceptions = MagicMock()
+        self.exceptions.NoSuchKey = _NoSuchKey
+
+    def _next_etag(self) -> str:
+        self._etag_seq += 1
+        return f'"{self._etag_seq}"'
+
+    def get_object(self, Bucket: str, Key: str) -> dict:
+        with self._lock:
+            obj_key = (Bucket, Key)
+            if obj_key not in self._objects:
+                raise _NoSuchKey()
+            body_str, etag = self._objects[obj_key]
+        body_mock = MagicMock()
+        body_mock.read.return_value = body_str.encode("utf-8")
+        return {"Body": body_mock, "ETag": etag}
+
+    def put_object(
+        self,
+        Bucket: str,
+        Key: str,
+        Body: bytes,
+        ContentType: str = "application/json",
+        IfMatch: str | None = None,
+    ) -> dict:
+        with self._lock:
+            obj_key = (Bucket, Key)
+            if IfMatch is not None and obj_key in self._objects:
+                _, current_etag = self._objects[obj_key]
+                if current_etag != IfMatch:
+                    raise _PreconditionFailed()
+            body_str = Body.decode("utf-8") if isinstance(Body, bytes) else Body
+            self._objects[obj_key] = (body_str, self._next_etag())
+        return {}
+
+
+# ---------------------------------------------------------------------------
+# Fixtures
+# ---------------------------------------------------------------------------
+
+
+@pytest.fixture
+def fake_client():
+    return _FakeS3Client()
+
+
+@pytest.fixture
+def backend(fake_client):
+    """S3Backend wired to the fake client (bypasses real boto3 session)."""
+    with patch("boto3.Session") as mock_session_cls:
+        mock_session = MagicMock()
+        mock_session.client.return_value = fake_client
+        mock_session_cls.return_value = mock_session
+        b = S3Backend(bucket="test-bucket", key="cache.json")
+    # Swap in our fake after construction
+    b._client = fake_client
+    return b
+
+
+@pytest.fixture
+def sample_cache():
+    return StepCache(
+        step_id="s3_node",
+        start_timestamp="2024-01-01T00:00:00+00:00",
+        end_timestamp="2024-01-01T00:00:01+00:00",
+        session_id="sess-s3",
+        worker_id="main",
+        code_hash="s3hash",
+        input_hash="inp",
+        parameter_hash="par",
+    )
+
+
+# ---------------------------------------------------------------------------
+# Basic CRUD
+# ---------------------------------------------------------------------------
+
+
+class TestS3BackendGetSet:
+    def test_get_returns_none_on_empty_bucket(self, backend):
+        assert backend.get("missing") is None
+
+    def test_set_and_get_roundtrip(self, backend, sample_cache):
+        backend.set("s3_node", sample_cache)
+        result = backend.get("s3_node")
+        assert result is not None
+        assert result.step_id == "s3_node"
+        assert result.code_hash == "s3hash"
+
+    def test_overwrite_entry(self, backend, sample_cache):
+        backend.set("s3_node", sample_cache)
+        updated = StepCache(**{**sample_cache.to_dict(), "code_hash": "new_s3_hash"})
+        backend.set("s3_node", updated)
+        result = backend.get("s3_node")
+        assert result.code_hash == "new_s3_hash"
+
+
+class TestS3BackendDelete:
+    def test_delete_removes_entry(self, backend, sample_cache):
+        backend.set("s3_node", sample_cache)
+        backend.delete("s3_node")
+        assert backend.get("s3_node") is None
+
+    def test_delete_nonexistent_is_noop(self, backend):
+        backend.delete("does_not_exist")  # must not raise
+
+
+class TestS3BackendPendingSentinel:
+    def test_pending_sentinel_hidden_from_get(self, backend):
+        backend.set_pending("node.__pending__")
+        assert backend.get("node.__pending__") is None
+
+    def test_set_pending_writes_to_store(self, backend, fake_client):
+        backend.set_pending("node.__pending__")
+        # Raw store should contain the sentinel
+        store, _ = backend._get_object()
+        assert "node.__pending__" in store
+
+
+# ---------------------------------------------------------------------------
+# atomic_update with ETag CAS
+# ---------------------------------------------------------------------------
+
+
+class TestS3BackendAtomicUpdate:
+    def test_atomic_update_increments_field(self, backend, sample_cache):
+        backend.set("s3_node", sample_cache)
+
+        def _inc(existing):
+            if existing is None:
+                return None
+            return StepCache(
+                **{**existing.to_dict(), "cache_hits": existing.cache_hits + 1}
+            )
+
+        backend.atomic_update("s3_node", _inc)
+        result = backend.get("s3_node")
+        assert result.cache_hits == 1
+
+    def test_atomic_update_deletes_when_updater_returns_none(
+        self, backend, sample_cache
+    ):
+        backend.set("s3_node", sample_cache)
+        backend.atomic_update("s3_node", lambda _: None)
+        assert backend.get("s3_node") is None
+
+    def test_atomic_update_on_missing_key_passes_none(self, backend):
+        received = []
+
+        def _updater(existing):
+            received.append(existing)
+
+        backend.atomic_update("missing", _updater)
+        assert received == [None]
+
+    def test_atomic_update_retries_on_etag_conflict(self, backend, fake_client):
+        """Simulate a concurrent writer that bumps the ETag between the
+        backend's read and conditional write.  The backend should retry and
+        eventually succeed."""
+        initial = StepCache(
+            step_id="contested",
+            start_timestamp="",
+            end_timestamp="",
+            session_id="",
+            worker_id="",
+            cache_hits=0,
+        )
+        backend.set("contested", initial)
+
+        # Intercept put_object to inject one conflict
+        original_put = fake_client.put_object
+        conflict_count = {"n": 0}
+
+        def _conflicting_put(**kwargs):
+            if kwargs.get("IfMatch") and conflict_count["n"] < 1:
+                conflict_count["n"] += 1
+                # Silently bump the ETag to simulate a concurrent write
+                store, _ = backend._get_object()
+                store["contested"]["cache_hits"] = 999  # noise
+                fake_client._objects[(backend._bucket, backend._key)] = (
+                    json.dumps(store),
+                    fake_client._next_etag(),
+                )
+                raise _PreconditionFailed()
+            return original_put(**kwargs)
+
+        fake_client.put_object = _conflicting_put
+
+        def _inc(existing):
+            if existing is None:
+                return None
+            return StepCache(
+                **{**existing.to_dict(), "cache_hits": existing.cache_hits + 1}
+            )
+
+        # Must not raise — retries should recover
+        backend.atomic_update("contested", _inc)
+        result = backend.get("contested")
+        # After the retry it read the noise value (999) and incremented
+        assert result.cache_hits == 1000
+
+
+# ---------------------------------------------------------------------------
+# Concurrency — multiple threads hammering atomic_update
+# ---------------------------------------------------------------------------
+
+
+class TestS3BackendConcurrency:
+    def test_concurrent_atomic_updates_no_lost_increments(self, backend, sample_cache):
+        initial = StepCache(
+            **{**sample_cache.to_dict(), "step_id": "shared", "cache_hits": 0}
+        )
+        backend.set("shared", initial)
+
+        n_threads = 30
+        errors: list[Exception] = []
+
+        def _increment():
+            try:
+
+                def _inc(existing):
+                    if existing is None:
+                        return None
+                    return StepCache(
+                        **{
+                            **existing.to_dict(),
+                            "cache_hits": existing.cache_hits + 1,
+                        }
+                    )
+
+                backend.atomic_update("shared", _inc)
+            except Exception as exc:
+                errors.append(exc)
+
+        threads = [threading.Thread(target=_increment) for _ in range(n_threads)]
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join()
+
+        assert errors == []
+        result = backend.get("shared")
+        assert result.cache_hits == n_threads
+
+
+# ---------------------------------------------------------------------------
+# Factory integration
+# ---------------------------------------------------------------------------
+
+
+class TestCreateRegistryS3:
+    def test_create_registry_s3_type(self):
+        with patch("boto3.Session") as mock_session_cls:
+            mock_session = MagicMock()
+            mock_session.client.return_value = MagicMock()
+            mock_session_cls.return_value = mock_session
+
+            from kedro.cache.registry import create_registry  # noqa: PLC0415
+
+            reg = create_registry("s3", bucket="my-bucket")
+        assert isinstance(reg, S3Backend)
+
+    def test_create_registry_unknown_type_raises(self):
+        from kedro.cache.registry import create_registry  # noqa: PLC0415
+
+        with pytest.raises(ValueError, match="Unknown cache backend"):
+            create_registry("dynamo")
diff --git a/tests/cache/test_step_cache.py b/tests/cache/test_step_cache.py
new file mode 100644
index 00000000..400fdc21
--- /dev/null
+++ b/tests/cache/test_step_cache.py
@@ -0,0 +1,274 @@
+"""Tests for StepCache dataclass, serialization, validation, and credential masking."""
+
+from __future__ import annotations
+
+import json
+
+import pytest
+
+from kedro.cache.step_cache import (
+    StepCache,
+    _diff_code_files,
+    _mask_cli_flags,
+    compute_input_hash,
+    compute_parameter_hash,
+)
+
+
+@pytest.fixture
+def sample_cache():
+    return StepCache(
+        step_id="test_node",
+        start_timestamp="2024-01-01T00:00:00+00:00",
+        end_timestamp="2024-01-01T00:00:01+00:00",
+        session_id="session-123",
+        worker_id="main",
+        runner_class="kedro.runner.SequentialRunner",
+        pipeline_namespace=None,
+        config_env="local",
+        code_hash="abc123",
+        input_hash="def456",
+        parameter_hash="ghi789",
+        input_paths={"dataset_a": "/data/a.parquet"},
+    )
+
+
+class TestStepCacheSerialization:
+    def test_to_dict_returns_valid_dict(self, sample_cache):
+        result = sample_cache.to_dict()
+        assert isinstance(result, dict)
+        assert result["step_id"] == "test_node"
+        assert result["code_hash"] == "abc123"
+
+    def test_from_dict_roundtrip(self, sample_cache):
+        data = sample_cache.to_dict()
+        restored = StepCache.from_dict(data)
+        assert restored == sample_cache
+
+    def test_to_json_returns_valid_json(self, sample_cache):
+        raw = sample_cache.to_json()
+        parsed = json.loads(raw)
+        assert parsed["step_id"] == "test_node"
+
+    def test_from_json_roundtrip(self, sample_cache):
+        raw = sample_cache.to_json()
+        restored = StepCache.from_json(raw)
+        assert restored == sample_cache
+
+
+class TestStepCacheMatches:
+    def test_identical_caches_match(self, sample_cache):
+        is_match, reason = sample_cache.matches(sample_cache)
+        assert is_match is True
+        assert reason is None
+
+    def test_code_hash_change_causes_miss(self, sample_cache):
+        other = StepCache(**{**sample_cache.to_dict(), "code_hash": "CHANGED"})
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "code_hash" in reason
+
+    def test_input_hash_change_causes_miss(self, sample_cache):
+        other = StepCache(**{**sample_cache.to_dict(), "input_hash": "CHANGED"})
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "input_hash" in reason
+
+    def test_parameter_hash_change_causes_miss(self, sample_cache):
+        other = StepCache(**{**sample_cache.to_dict(), "parameter_hash": "CHANGED"})
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "parameter_hash" in reason
+
+    def test_runner_class_change_causes_miss(self, sample_cache):
+        other = StepCache(**{**sample_cache.to_dict(), "runner_class": "OtherRunner"})
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "runner_class" in reason
+
+    def test_config_env_change_causes_miss(self, sample_cache):
+        other = StepCache(**{**sample_cache.to_dict(), "config_env": "production"})
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "config_env" in reason
+
+    def test_namespace_change_causes_miss(self, sample_cache):
+        other = StepCache(**{**sample_cache.to_dict(), "pipeline_namespace": "new_ns"})
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "pipeline_namespace" in reason
+
+    def test_input_paths_key_change_causes_miss(self, sample_cache):
+        other = StepCache(
+            **{**sample_cache.to_dict(), "input_paths": {"new_key": "/path"}}
+        )
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is False
+        assert "input_paths" in reason
+
+    def test_metadata_fields_do_not_cause_miss(self, sample_cache):
+        other = StepCache(
+            **{
+                **sample_cache.to_dict(),
+                "session_id": "different-session",
+                "worker_id": "Worker-99",
+                "start_timestamp": "2099-01-01T00:00:00+00:00",
+                "cache_hits": 42,
+            }
+        )
+        is_match, reason = sample_cache.matches(other)
+        assert is_match is True
+        assert reason is None
+
+
+class TestCredentialMasking:
+    def test_masks_password(self):
+        flags = {"password": "secret123", "verbose": True}
+        result = _mask_cli_flags(flags)
+        assert result["password"] == "***REDACTED***"  # noqa: S105
+        assert result["verbose"] is True
+
+    def test_masks_api_key(self):
+        flags = {"api_key": "sk-abc123"}
+        result = _mask_cli_flags(flags)
+        assert result["api_key"] == "***REDACTED***"
+
+    def test_masks_token(self):
+        flags = {"auth_token": "bearer-xyz"}
+        result = _mask_cli_flags(flags)
+        assert result["auth_token"] == "***REDACTED***"  # noqa: S105
+
+    def test_case_insensitive_matching(self):
+        flags = {"PASSWORD": "secret", "SECRET_KEY": "key", "normal": "ok"}
+        result = _mask_cli_flags(flags)
+        assert result["PASSWORD"] == "***REDACTED***"  # noqa: S105
+        assert result["SECRET_KEY"] == "***REDACTED***"  # noqa: S105
+        assert result["normal"] == "ok"
+
+    def test_empty_flags_unchanged(self):
+        assert _mask_cli_flags({}) == {}
+
+    def test_no_sensitive_keys_unchanged(self):
+        flags = {"verbose": True, "env": "local"}
+        assert _mask_cli_flags(flags) == flags
+
+
+class TestHashFunctions:
+    def test_compute_input_hash_deterministic(self):
+        inputs = {"a": 1, "b": [1, 2, 3]}
+        h1 = compute_input_hash(inputs)
+        h2 = compute_input_hash(inputs)
+        assert h1 == h2
+
+    def test_compute_input_hash_order_independent(self):
+        h1 = compute_input_hash({"a": 1, "b": 2})
+        h2 = compute_input_hash({"b": 2, "a": 1})
+        assert h1 == h2
+
+    def test_compute_input_hash_different_values(self):
+        h1 = compute_input_hash({"a": 1})
+        h2 = compute_input_hash({"a": 2})
+        assert h1 != h2
+
+    def test_compute_parameter_hash_deterministic(self):
+        params = {"lr": 0.01, "epochs": 10}
+        h1 = compute_parameter_hash(params)
+        h2 = compute_parameter_hash(params)
+        assert h1 == h2
+
+    def test_compute_parameter_hash_different_params(self):
+        h1 = compute_parameter_hash({"lr": 0.01})
+        h2 = compute_parameter_hash({"lr": 0.001})
+        assert h1 != h2
+
+
+class TestDiffCodeFiles:
+    def test_both_empty_returns_no_detail(self):
+        result = _diff_code_files({}, {})
+        assert "no per-file detail available" in result
+
+    def test_one_side_empty_returns_unavailable(self):
+        assert "unavailable on one side" in _diff_code_files({"a.py": "h1"}, {})
+        assert "unavailable on one side" in _diff_code_files({}, {"a.py": "h1"})
+
+    def test_detects_changed_file(self):
+        stored = {"utils.py": "aaa", "main.py": "bbb"}
+        current = {"utils.py": "ccc", "main.py": "bbb"}
+        result = _diff_code_files(stored, current)
+        assert "changed in utils.py" in result
+        assert "main.py" not in result
+
+    def test_detects_added_file(self):
+        stored = {"main.py": "aaa"}
+        current = {"main.py": "aaa", "new_mod.py": "bbb"}
+        result = _diff_code_files(stored, current)
+        assert "new file new_mod.py" in result
+
+    def test_detects_removed_file(self):
+        stored = {"main.py": "aaa", "old_mod.py": "bbb"}
+        current = {"main.py": "aaa"}
+        result = _diff_code_files(stored, current)
+        assert "removed old_mod.py" in result
+
+    def test_combined_changes(self):
+        stored = {"a.py": "h1", "b.py": "h2", "c.py": "h3"}
+        current = {"a.py": "h1_changed", "c.py": "h3", "d.py": "h4"}
+        result = _diff_code_files(stored, current)
+        assert "changed in a.py" in result
+        assert "new file d.py" in result
+        assert "removed b.py" in result
+
+    def test_all_files_match_fallback(self):
+        same = {"a.py": "h1", "b.py": "h2"}
+        result = _diff_code_files(same, same)
+        assert "all files match" in result
+
+
+class TestCodeHashMissWithFileDetail:
+    """matches() should embed file-level detail when code_hash differs."""
+
+    def test_miss_reason_names_changed_file(self, sample_cache):
+        stored = StepCache(
+            **{
+                **sample_cache.to_dict(),
+                "code_files": {"utils.py": "aaa", "main.py": "bbb"},
+            }
+        )
+        current = StepCache(
+            **{
+                **sample_cache.to_dict(),
+                "code_hash": "DIFFERENT",
+                "code_files": {"utils.py": "ccc", "main.py": "bbb"},
+            }
+        )
+        is_match, reason = stored.matches(current)
+        assert is_match is False
+        assert "code_hash changed" in reason
+        assert "changed in utils.py" in reason
+
+    def test_miss_reason_names_new_file(self, sample_cache):
+        stored = StepCache(
+            **{
+                **sample_cache.to_dict(),
+                "code_files": {"main.py": "aaa"},
+            }
+        )
+        current = StepCache(
+            **{
+                **sample_cache.to_dict(),
+                "code_hash": "DIFFERENT",
+                "code_files": {"main.py": "aaa", "helpers.py": "bbb"},
+            }
+        )
+        is_match, reason = stored.matches(current)
+        assert is_match is False
+        assert "new file helpers.py" in reason
+
+    def test_miss_reason_fallback_when_no_code_files(self, sample_cache):
+        stored = StepCache(**{**sample_cache.to_dict(), "code_files": {}})
+        current = StepCache(
+            **{**sample_cache.to_dict(), "code_hash": "DIFFERENT", "code_files": {}}
+        )
+        is_match, reason = stored.matches(current)
+        assert is_match is False
+        assert "no per-file detail available" in reason
